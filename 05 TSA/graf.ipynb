{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyP2xelpDZIPuI1n0L+9i8Kb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","---\n","\n","# üîπ Handling Missing Values in Time Series Data\n","\n","Since **time order matters**, we can‚Äôt just blindly impute like normal tabular data. Here are the main techniques:\n","\n","---\n","\n","## 1. **Drop Missing Values**\n","\n","```python\n","df.dropna(inplace=True)\n","```\n","\n","* **Explanation**: Remove missing rows.\n","* ‚úÖ Advantage: Simple, no assumption.\n","* ‚ùå Disadvantage: Loses data, risky if gaps are big.\n","\n","---\n","\n","## 2. **Forward Fill (ffill)**\n","\n","```python\n","df.fillna(method='ffill', inplace=True)\n","```\n","\n","* **Explanation**: Replace missing value with **previous known value**.\n","* ‚úÖ Advantage: Good for continuous signals (stock price, sensor).\n","* ‚ùå Disadvantage: Can extend old value too far, not good for rapid changes.\n","\n","---\n","\n","## 3. **Backward Fill (bfill)**\n","\n","```python\n","df.fillna(method='bfill', inplace=True)\n","```\n","\n","* **Explanation**: Replace missing value with **next known value**.\n","* ‚úÖ Advantage: Easy, sometimes realistic.\n","* ‚ùå Disadvantage: Uses ‚Äúfuture‚Äù info ‚Üí not valid for forecasting.\n","\n","---\n","\n","## 4. **Linear Interpolation**\n","\n","```python\n","df.interpolate(method='linear', inplace=True)\n","```\n","\n","* **Explanation**: Draw a straight line between neighboring points.\n","* ‚úÖ Advantage: Smooth, good for gradual trends.\n","* ‚ùå Disadvantage: Wrong if data has sudden jumps.\n","\n","---\n","\n","## 5. **Time-based Interpolation**\n","\n","```python\n","df.interpolate(method='time', inplace=True)\n","```\n","\n","* **Explanation**: Uses time index ‚Üí better for irregular intervals.\n","* ‚úÖ Advantage: More realistic for uneven time steps.\n","* ‚ùå Disadvantage: Still assumes smoothness.\n","\n","---\n","\n","## 6. **Mean/Median/Mode Imputation**\n","\n","```python\n","df['value'].fillna(df['value'].mean(), inplace=True)\n","```\n","\n","* **Explanation**: Replace with average of entire series.\n","* ‚úÖ Advantage: Very simple.\n","* ‚ùå Disadvantage: Ignores seasonality & time trend.\n","\n","---\n","\n","## 7. **Moving Average (Rolling Window)**\n","\n","```python\n","df['value'].fillna(df['value'].rolling(3, min_periods=1).mean(), inplace=True)\n","```\n","\n","* **Explanation**: Replace with local average (last 3 values here).\n","* ‚úÖ Advantage: Smooths noise, respects local structure.\n","* ‚ùå Disadvantage: Can oversmooth, lags real trend.\n","\n","---\n","\n","## 8. **Seasonal/Trend Decomposition**\n","\n","```python\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","# decompose into trend, seasonality, residuals ‚Üí fill gaps\n","```\n","\n","* **Explanation**: Estimate missing values using seasonal/trend pattern.\n","* ‚úÖ Advantage: Good for seasonal data (daily, monthly).\n","* ‚ùå Disadvantage: Needs lots of data, more complex.\n","\n","---\n","\n","## 9. **Model-based Imputation**\n","\n","```python\n","from sklearn.linear_model import LinearRegression\n","# predict missing values using regression, ARIMA, or ML models\n","```\n","\n","* **Explanation**: Predict missing points using statistical/ML models.\n","* ‚úÖ Advantage: Captures dependencies & patterns.\n","* ‚ùå Disadvantage: Computationally expensive, risk of overfitting.\n","\n","---\n","\n","# ‚úÖ Quick Summary Table\n","\n","| Method                  | Best for                 | Advantage     | Disadvantage               |\n","| ----------------------- | ------------------------ | ------------- | -------------------------- |\n","| Drop Missing            | Few missing points       | Simple        | Data loss                  |\n","| Forward Fill            | Continuous signals       | Easy          | Extends old values too far |\n","| Backward Fill           | Rare gaps                | Simple        | Uses ‚Äúfuture‚Äù info         |\n","| Linear Interpolation    | Gradual changes          | Smooth        | Bad for sudden jumps       |\n","| Time Interpolation      | Irregular time intervals | Accurate      | Assumes smoothness         |\n","| Mean/Median Imputation  | Few missing values       | Simple        | Ignores trend/season       |\n","| Moving Average          | Local smoothing          | Reduces noise | Oversmoothing              |\n","| Seasonal/Trend Fill     | Strong seasonality       | Accurate      | Complex                    |\n","| Model-based (ARIMA, ML) | Large gaps, complex data | Smart         | Expensive                  |\n","\n","---\n","\n","üëâ Rule of Thumb:\n","\n","* **Small gaps** ‚Üí Forward/Backward fill, Interpolation.\n","* **Seasonal data** ‚Üí Seasonal decomposition.\n","* **Big gaps** ‚Üí Model-based imputation.\n","* **Very few missing values** ‚Üí Drop rows.\n","\n","---\n","\n"],"metadata":{"id":"Ks52pXQhndvj"}},{"cell_type":"markdown","source":["Great question üëç ‚Äî **ACF (Autocorrelation Function)** and **PACF (Partial Autocorrelation Function)** are two of the most important tools in **time series analysis**, especially when working with **AR, MA, ARMA, ARIMA models**. These are almost always asked in **interviews** for time series-related roles. Let‚Äôs go deep üëá\n","\n","---\n","\n","# 1. **Autocorrelation Function (ACF)**\n","\n","* **Definition**: Measures the correlation between the time series and its **lagged versions**.\n","* Simply: *How much does today‚Äôs value depend on yesterday‚Äôs, the day before, etc.?*\n","* Formula:\n","\n","  $$\n","  ACF(k) = Corr(Y_t, Y_{t-k})\n","  $$\n","\n","  where $k$ is the lag.\n","\n","‚úÖ **Key use**: Helps identify the **order of MA (q)** component in ARIMA.\n","\n","---\n","\n","### Example intuition:\n","\n","If today‚Äôs stock price is highly correlated with yesterday‚Äôs price, and less with the day before, the **ACF** will show a slow decay pattern.\n","\n","---\n","\n","# 2. **Partial Autocorrelation Function (PACF)**\n","\n","* **Definition**: Measures the correlation between the series and its **lag**, after removing the effect of all intermediate lags.\n","* Simply: *What is the ‚Äúdirect‚Äù effect of lag $k$, ignoring the effect of lag $1, 2, ‚Ä¶ k-1$?*\n","* Formula (conceptual): Residual correlation after controlling for intermediate lags.\n","\n","‚úÖ **Key use**: Helps identify the **order of AR (p)** component in ARIMA.\n","\n","---\n","\n","### Example intuition:\n","\n","Suppose today‚Äôs value depends on yesterday‚Äôs value. Naturally, it will also seem correlated with the day before yesterday (through yesterday).\n","\n","* **ACF** will show both correlations.\n","* **PACF** will filter out the ‚Äúindirect‚Äù correlation and keep only the direct one.\n","\n","---\n","\n","# 3. **Graphical Patterns for Model Selection**\n","\n","When you plot ACF and PACF, you use them to determine **p** and **q** in ARIMA:\n","\n","* **AR(p) process**\n","\n","  * PACF: Cuts off after lag *p*.\n","  * ACF: Tails off gradually.\n","\n","* **MA(q) process**\n","\n","  * ACF: Cuts off after lag *q*.\n","  * PACF: Tails off gradually.\n","\n","* **ARMA(p,q) process**\n","\n","  * Both ACF and PACF tail off gradually.\n","\n","---\n","\n","# 4. **Simple Code (Snapshot)**\n","\n","```python\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","import matplotlib.pyplot as plt\n","\n","# Assuming 'data' is your time series\n","plot_acf(data, lags=30)\n","plot_pacf(data, lags=30)\n","plt.show()\n","```\n","\n","---\n","\n","# 5. **Advantages & Disadvantages**\n","\n","### ‚úÖ Advantages:\n","\n","* Easy diagnostic tool for ARIMA model selection.\n","* Helps separate AR vs MA processes.\n","* Provides intuition about temporal dependencies.\n","\n","### ‚ùå Disadvantages:\n","\n","* Can be misleading if the series is **non-stationary** (spurious correlations).\n","* Noise in data makes interpretation tricky.\n","* Higher-order dependencies are hard to spot visually.\n","\n","---\n","\n","# 6. **Interview Tip**\n","\n","If asked: *‚ÄúWhy do we need both ACF and PACF?‚Äù*\n","üëâ Answer: Because ACF captures total correlation (direct + indirect), while PACF isolates **direct correlation** at each lag. Together, they allow us to identify whether the time series is better modeled by AR, MA, or ARMA.\n","\n","---\n","\n","Would you like me to also make a **cheat sheet visualization** of ACF vs PACF patterns for AR, MA, ARMA (like what interviewers expect you to sketch on a whiteboard)?\n"],"metadata":{"id":"VvAtByB1vWk2"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGNe1y2AnDKu"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["\n","---\n","\n","# üîπ Moving Averages (MA) for Missing Values\n","\n","A **moving average** smooths data by replacing a missing value with the average of nearby values (past, future, or both).\n","\n","---\n","\n","## 1. **Simple Moving Average (SMA)**\n","\n","üëâ Replace missing value with the average of values in a fixed-size **window**.\n","\n","```python\n","df['value'] = df['value'].fillna(df['value'].rolling(window=3, min_periods=1).mean())\n","```\n","\n","* **Explanation**: Takes the mean of the last 3 values (you can choose window size).\n","* **Advantage**: Smooths noise, simple, works well for short gaps.\n","* **Disadvantage**: Can lag behind real trend, not good for highly volatile data.\n","\n","---\n","\n","## 2. **Centered Moving Average**\n","\n","üëâ Uses both past and future values to fill missing points.\n","\n","```python\n","df['value'] = df['value'].fillna(df['value'].rolling(window=3, center=True, min_periods=1).mean())\n","```\n","\n","* **Explanation**: Looks at neighbors on both sides of the missing value.\n","* **Advantage**: Balances forward and backward trends.\n","* **Disadvantage**: Not possible at the edges (start or end of series).\n","\n","---\n","\n","## 3. **Exponential Moving Average (EMA)**\n","\n","üëâ More weight to **recent observations** instead of equal weight.\n","\n","```python\n","df['value'] = df['value'].fillna(df['value'].ewm(span=3, adjust=False).mean())\n","```\n","\n","* **Explanation**: Recent values are considered more important.\n","* **Advantage**: Better for financial data, captures quick changes.\n","* **Disadvantage**: Still a smoothing method ‚Üí may distort sudden spikes.\n","\n","---\n","\n","‚úÖ **Summary of Moving Averages for Missing Values**\n","\n","| Method          | When to Use             | Advantage               | Disadvantage                 |\n","| --------------- | ----------------------- | ----------------------- | ---------------------------- |\n","| **SMA**         | Short gaps, stable data | Simple, smooths noise   | Lags, may oversmooth         |\n","| **Centered MA** | Symmetric missing data  | Balanced trend filling  | Not for edges                |\n","| **EMA**         | Financial/volatile data | Captures recent changes | May ignore long-term pattern |\n","\n","---\n","\n","üëâ Moving averages are best when:\n","\n","* Data is **continuous** and not too seasonal.\n","* Gaps are **short** (few missing points).\n","* You want **smoothed values** instead of exact estimates.\n","\n","---\n","\n","Do you want me to also show how **moving averages compare with interpolation** visually (with a quick Python plot)?\n"],"metadata":{"id":"0VAsgr55nDuv"}},{"cell_type":"code","source":[],"metadata":{"id":"PWeNBWKznMaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perfect üëå ‚Äî let‚Äôs break down **Time Series Stationarity Testing** with the **ADF (Augmented Dickey-Fuller)** and **KPSS (Kwiatkowski‚ÄìPhillips‚ÄìSchmidt‚ÄìShin)** tests.\n","\n","---\n","\n","# üîπ Why Test for Stationarity?\n","\n","Most time series models (like **ARIMA, SARIMA**) assume **stationarity**:\n","\n","* Mean, variance, and autocorrelation are **constant over time**.\n","\n","If the series is **non-stationary** (has trend/seasonality), forecasts will be unreliable.\n","So, we use **statistical tests**.\n","\n","---\n","\n","# üîπ 1. Augmented Dickey-Fuller (ADF) Test\n","\n","üëâ Tests for **unit root** (non-stationarity).\n","\n","### Hypotheses:\n","\n","* **Null (H‚ÇÄ):** Series has a unit root ‚Üí **non-stationary**.\n","* **Alternative (H‚ÇÅ):** Series is stationary.\n","\n","### Python Example:\n","\n","```python\n","from statsmodels.tsa.stattools import adfuller\n","\n","result = adfuller(df['value'])\n","print('ADF Statistic:', result[0])\n","print('p-value:', result[1])\n","```\n","\n","### Interpretation:\n","\n","* **p-value < 0.05** ‚Üí Reject H‚ÇÄ ‚Üí Series is **stationary**.\n","* **p-value ‚â• 0.05** ‚Üí Fail to reject H‚ÇÄ ‚Üí Series is **non-stationary**.\n","\n","‚úÖ **Good for detecting non-stationarity due to trend.**\n","‚ùå **Weak when series has strong seasonality.**\n","\n","---\n","\n","# üîπ 2. KPSS Test\n","\n","üëâ Tests for **trend stationarity** (opposite of ADF).\n","\n","### Hypotheses:\n","\n","* **Null (H‚ÇÄ):** Series is stationary (trend-stationary).\n","* **Alternative (H‚ÇÅ):** Series is **non-stationary**.\n","\n","### Python Example:\n","\n","```python\n","from statsmodels.tsa.stattools import kpss\n","\n","result = kpss(df['value'], regression='c', nlags=\"auto\")\n","print('KPSS Statistic:', result[0])\n","print('p-value:', result[1])\n","```\n","\n","### Interpretation:\n","\n","* **p-value < 0.05** ‚Üí Reject H‚ÇÄ ‚Üí Series is **non-stationary**.\n","* **p-value ‚â• 0.05** ‚Üí Fail to reject H‚ÇÄ ‚Üí Series is **stationary**.\n","\n","‚úÖ **Better at catching trend-stationarity**.\n","‚ùå Can over-reject in small samples.\n","\n","---\n","\n","# üîπ Using Both Together\n","\n","Since ADF and KPSS test **opposite null hypotheses**:\n","\n","* **ADF rejects, KPSS does not reject ‚Üí Stationary.**\n","* **ADF does not reject, KPSS rejects ‚Üí Non-stationary.**\n","* **Both reject ‚Üí Inconclusive (may need transformation).**\n","* **Both do not reject ‚Üí Inconclusive (try differencing/transforming).**\n","\n","---\n","\n","# üîπ Summary Table\n","\n","| Test     | Null Hypothesis (H‚ÇÄ)     | If p < 0.05    | If p ‚â• 0.05    |\n","| -------- | ------------------------ | -------------- | -------------- |\n","| **ADF**  | Series is non-stationary | Stationary     | Non-stationary |\n","| **KPSS** | Series is stationary     | Non-stationary | Stationary     |\n","\n","---\n","\n","‚úÖ **Rule of Thumb**:\n","\n","* Always use **both ADF & KPSS** to confirm.\n","* If series is **non-stationary**, fix it using:\n","\n","  * **Differencing** (remove trend).\n","  * **Seasonal differencing**.\n","  * **Log/Box-Cox transformation** (stabilize variance).\n","\n","---\n","\n","Would you like me to also show a **step-by-step Python example** where we run both ADF & KPSS on a dataset (like Airline Passengers) and then make it stationary with differencing?\n"],"metadata":{"id":"U189pdsurCL2"}},{"cell_type":"code","source":[],"metadata":{"id":"Y7pj4n09rDVe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perfect üëå ‚Äî let‚Äôs go into **AR, MA, ARMA, and ARIMA** in **master-level depth** so you can handle **interview questions** with confidence.\n","\n","---\n","\n","# üîπ 1. **AR (AutoRegressive) Model**\n","\n","### Idea:\n","\n","* Current value of the time series depends **linearly** on its **past values**.\n","* Captures **momentum & persistence** in the series.\n","\n","**Equation (AR(p)):**\n","\n","$$\n","y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\cdots + \\phi_p y_{t-p} + \\epsilon_t\n","$$\n","\n","* $y_t$: current value\n","* $y_{t-1}, y_{t-2}, ‚Ä¶$: past values\n","* $\\phi_i$: AR coefficients\n","* $\\epsilon_t$: white noise\n","\n","üìå Example: Stock prices, temperature (where today depends on yesterday + previous days).\n","\n","‚úÖ **Advantage**: Great at capturing **trend and persistence**.\n","‚ùå **Disadvantage**: Can fail if there is **seasonality** or irregular shocks.\n","\n","---\n","\n","# üîπ 2. **MA (Moving Average) Model**\n","\n","### Idea:\n","\n","* Current value depends on **past forecast errors (shocks/noise)**.\n","* Captures **short-term shocks**.\n","\n","**Equation (MA(q)):**\n","\n","$$\n","y_t = c + \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\cdots + \\theta_q \\epsilon_{t-q}\n","$$\n","\n","* $\\epsilon_t$: white noise\n","* $\\theta_i$: MA coefficients\n","\n","üìå Example: Demand fluctuations influenced by **sudden external shocks**.\n","\n","‚úÖ **Advantage**: Handles **random shocks** well.\n","‚ùå **Disadvantage**: Cannot capture long memory or trend.\n","\n","---\n","\n","# üîπ 3. **ARMA (AutoRegressive Moving Average) Model**\n","\n","### Idea:\n","\n","* Combines **AR (dependence on past values)** and **MA (dependence on shocks)**.\n","* Works for **stationary** series.\n","\n","**Equation (ARMA(p,q)):**\n","\n","$$\n","y_t = c + \\sum_{i=1}^p \\phi_i y_{t-i} + \\epsilon_t + \\sum_{j=1}^q \\theta_j \\epsilon_{t-j}\n","$$\n","\n","üìå Example: Financial returns, rainfall data (stationary, no trend).\n","\n","‚úÖ **Advantage**: Captures both **memory + shocks**.\n","‚ùå **Disadvantage**: Requires **stationarity** (no trend/seasonality).\n","\n","---\n","\n","# üîπ 4. **ARIMA (AutoRegressive Integrated Moving Average)**\n","\n","### Idea:\n","\n","* Extension of ARMA for **non-stationary** series.\n","* Adds a **differencing step** to remove trend.\n","\n","**Equation (ARIMA(p,d,q)):**\n","\n","* $p$: order of AR\n","* $d$: differencing order (number of times differenced to achieve stationarity)\n","* $q$: order of MA\n","\n","**Form:**\n","\n","$$\n","(1 - \\sum_{i=1}^p \\phi_i L^i)(1-L)^d y_t = (1 + \\sum_{j=1}^q \\theta_j L^j)\\epsilon_t\n","$$\n","\n","Where $L$ is the **lag operator**.\n","\n","üìå Example: Stock market indices, GDP, sales (with trend/seasonality removed).\n","\n","‚úÖ **Advantage**: Works for **non-stationary** series.\n","‚ùå **Disadvantage**: Doesn‚Äôt directly handle **seasonality** ‚Üí need **SARIMA**.\n","\n","---\n","\n","# üîπ How to Choose Between AR, MA, ARMA, ARIMA?\n","\n","* **Autocorrelation Function (ACF)** & **Partial Autocorrelation Function (PACF)** are key:\n","\n","  * **AR(p):** PACF cuts off after lag p, ACF decays gradually.\n","  * **MA(q):** ACF cuts off after lag q, PACF decays gradually.\n","  * **ARMA(p,q):** Both ACF & PACF decay gradually.\n","  * **ARIMA:** If series is non-stationary ‚Üí difference first.\n","\n","---\n","\n","# üîπ Interview-Level Insights\n","\n","‚úÖ **Q1. How do you identify AR vs MA vs ARMA?**\n","\n","* By looking at ACF/PACF plots.\n","\n","‚úÖ **Q2. Why ARIMA over ARMA?**\n","\n","* ARMA requires stationarity, ARIMA allows differencing.\n","\n","‚úÖ **Q3. What if seasonality exists?**\n","\n","* Use **SARIMA / Seasonal ARIMA**.\n","\n","‚úÖ **Q4. Limitations of ARIMA?**\n","\n","* Assumes linearity, poor for long-term forecasting with regime changes.\n","\n","‚úÖ **Q5. Alternatives to ARIMA?**\n","\n","* **Exponential smoothing (ETS), Prophet (Facebook), State-Space Models, LSTMs (Deep Learning)**.\n","\n","---\n","\n","üëâ Do you want me to also create a **step-by-step comparison table** (AR vs MA vs ARMA vs ARIMA) with **ACF/PACF patterns + use cases + pros/cons**? That will be interview gold.\n"],"metadata":{"id":"HJiP_CF0t1E1"}},{"cell_type":"code","source":[],"metadata":{"id":"QZ1RO1YCt1bg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Got it üëç Let‚Äôs go **step by step** and make anomaly detection & autoencoders very clear (both theory + intuition + math + use cases).\n","\n","---\n","\n","# üîπ 1. What is **Anomaly Detection**?\n","\n","### Definition\n","\n","Anomaly detection is the task of identifying **data points that deviate significantly from the majority of the data**. These unusual data points are called **anomalies** or **outliers**.\n","\n","### Why important?\n","\n","* Fraud detection (bank transactions, credit card misuse)\n","* Intrusion detection (network traffic)\n","* Fault detection (manufacturing, IoT sensors)\n","* Healthcare (rare diseases in medical scans)\n","* Predictive maintenance (detecting unusual machine vibrations)\n","\n","---\n","\n","### Types of Anomalies:\n","\n","1. **Point Anomalies**\n","\n","   * A single data point far away from the rest.\n","   * Example: A transaction of \\$100,000 when typical spending is \\$500.\n","\n","2. **Contextual Anomalies**\n","\n","   * Normal in one context, abnormal in another.\n","   * Example: \\$200 electricity bill in summer (normal), but same in winter (anomaly).\n","\n","3. **Collective Anomalies**\n","\n","   * A group of related points is abnormal.\n","   * Example: A sudden spike in server requests (possible DDoS attack).\n","\n","---\n","\n","### Approaches for Anomaly Detection:\n","\n","1. **Statistical Methods**\n","\n","   * Assume data follows distribution (e.g., Gaussian).\n","   * If probability is very low ‚Üí anomaly.\n","   * Example: Z-score, Grubbs‚Äô test.\n","\n","2. **Distance-based Methods**\n","\n","   * If a point is too far from neighbors ‚Üí anomaly.\n","   * Example: k-NN, Mahalanobis distance.\n","\n","3. **Density-based Methods**\n","\n","   * Look at local density of data.\n","   * If a point is in a sparse region ‚Üí anomaly.\n","   * Example: LOF (Local Outlier Factor).\n","\n","4. **Machine Learning Methods**\n","\n","   * Supervised: Train classifier with ‚Äúnormal‚Äù vs ‚Äúanomaly‚Äù labels.\n","   * Unsupervised: Assume anomalies are rare and different from normal.\n","   * Semi-supervised: Train only on ‚Äúnormal‚Äù data, detect anomalies at test time.\n","\n","5. **Deep Learning Methods**\n","\n","   * **Autoencoders, LSTM, GANs** ‚Üí useful for high-dimensional & complex data.\n","\n","---\n","\n","# üîπ 2. What is an **Autoencoder**?\n","\n","### Definition\n","\n","An **Autoencoder (AE)** is a type of **unsupervised neural network** used to learn an efficient, compressed representation of data.\n","\n","It has two parts:\n","\n","1. **Encoder** ‚Üí Compress input data into a smaller **latent space** (feature vector).\n","2. **Decoder** ‚Üí Reconstructs input from latent space.\n","\n","The goal: **Output ‚âà Input**\n","\n","---\n","\n","### Architecture\n","\n","```\n","Input ‚Üí [Encoder] ‚Üí Latent Representation ‚Üí [Decoder] ‚Üí Reconstructed Output\n","```\n","\n","* Encoder: Reduces dimensionality. (Dense / CNN / LSTM layers)\n","* Latent Space: Compressed vector (bottleneck).\n","* Decoder: Expands back to original size.\n","\n","---\n","\n","### Loss Function\n","\n","* For continuous data: **Mean Squared Error (MSE)**\n","  $L = \\frac{1}{N}\\sum (x - \\hat{x})^2$\n","* For binary data: **Binary Cross-Entropy**\n","\n","---\n","\n","### Why useful?\n","\n","* Forces the model to **learn key features** instead of memorizing.\n","* Learns compressed patterns of \"normal\" data.\n","\n","---\n","\n","# üîπ 3. **Autoencoders for Anomaly Detection**\n","\n","The intuition is simple:\n","üëâ Train Autoencoder **only on normal data**.\n","üëâ During inference, pass a new data point:\n","\n","* If it‚Äôs normal ‚Üí autoencoder reconstructs well (low error).\n","* If it‚Äôs anomaly ‚Üí reconstruction is poor (high error).\n","\n","Thus, **Reconstruction Error** acts as an anomaly score.\n","\n","---\n","\n","### Steps:\n","\n","1. Collect normal data (majority).\n","2. Train Autoencoder ‚Üí minimize reconstruction error.\n","3. For new data:\n","\n","   * Compute error = |x - reconstructed\\_x|.\n","   * If error > threshold ‚Üí anomaly.\n","\n","---\n","\n","### Example\n","\n","* Suppose we train an autoencoder on ECG signals of healthy patients.\n","* When fed abnormal heartbeat patterns ‚Üí the reconstruction is poor.\n","* High error ‚Üí detected as anomaly (possible disease).\n","\n","---\n","\n","# üîπ 4. Types of Autoencoders used in Anomaly Detection\n","\n","1. **Vanilla Autoencoder**\n","\n","   * Fully connected encoder-decoder.\n","   * Works for tabular data.\n","\n","2. **Convolutional Autoencoder (CAE)**\n","\n","   * Encoder: Conv layers.\n","   * Decoder: Deconv layers.\n","   * Works well for images & video.\n","\n","3. **Recurrent Autoencoder**\n","\n","   * Encoder: LSTM/GRU.\n","   * Decoder: LSTM/GRU.\n","   * Works well for time-series (e.g., stock prices, sensor data).\n","\n","4. **Variational Autoencoder (VAE)**\n","\n","   * Learns probability distribution of data (not just compression).\n","   * Can generate samples + detect anomalies.\n","\n","---\n","\n","# üîπ 5. Pros & Cons\n","\n","‚úÖ Pros:\n","\n","* Works in high-dimensional data (images, time series).\n","* No need for labeled anomalies.\n","* Learns complex patterns.\n","\n","‚ùå Cons:\n","\n","* Needs lots of normal data.\n","* Sensitive to hyperparameters (latent size, threshold).\n","* May reconstruct anomalies too well if trained improperly.\n","\n","---\n","\n","# üîπ 6. Real-World Applications\n","\n","* **Finance**: Fraud detection in transactions.\n","* **Cybersecurity**: Intrusion detection in network traffic.\n","* **Healthcare**: Detecting tumors or rare diseases from scans.\n","* **IoT & Manufacturing**: Predictive maintenance.\n","* **Retail**: Detect unusual customer behaviors.\n","\n","---\n","\n","‚ö°Quick Recap:\n","\n","* **Anomaly detection** = identifying unusual points.\n","* **Autoencoders** = compress + reconstruct data.\n","* For anomaly detection ‚Üí Train AE on **normal data** ‚Üí anomalies have **high reconstruction error**.\n","\n","---\n","\n","Do you want me to also give you a **step-by-step Python implementation (with PyTorch or TensorFlow)** for anomaly detection using autoencoders, so you can connect theory ‚Üí practice?\n"],"metadata":{"id":"uYlBT3IzyZCY"}},{"cell_type":"code","source":[],"metadata":{"id":"zEQEFkL_yZVs"},"execution_count":null,"outputs":[]}]}