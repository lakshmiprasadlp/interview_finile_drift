{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNkGQ3tTo6yTmaxi90yhWcp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"5-iBBveZuZNL","executionInfo":{"status":"ok","timestamp":1753517180550,"user_tz":-330,"elapsed":23751,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"outputs":[],"source":["%%capture\n","!pip install -U langchain-openai langchain_community langchain langchain_core langchain_openai chromadb"]},{"cell_type":"code","source":["from langchain_openai.chat_models.base import ChatOpenAI"],"metadata":{"id":"z6o-Al6Quunl","executionInfo":{"status":"ok","timestamp":1753516593509,"user_tz":-330,"elapsed":10,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0,api_key=userdata.get('lp'))\n"],"metadata":{"id":"-9hfAckRu5-C","executionInfo":{"status":"ok","timestamp":1753516666568,"user_tz":-330,"elapsed":1440,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["result = llm.invoke(\"Write a ballad about LangChain\")\n","print(result.content[:5])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DzV9v1KCvOe4","executionInfo":{"status":"ok","timestamp":1753516758006,"user_tz":-330,"elapsed":2127,"user":{"displayName":"200 project","userId":"16889346664957907140"}},"outputId":"ba2f10b6-f025-4432-b2a3-c46b16594525"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["In a \n"]}]},{"cell_type":"code","source":["from langchain_openai import OpenAIEmbeddings\n","embeddings = OpenAIEmbeddings(\n","    model=\"text-embedding-3-small\",\n","    api_key=userdata.get('lp')\n","    # With the `text-embedding-3` class\n","    # of models, you can specify the size\n","    # of the embeddings you want returned.\n","    # dimensions=1024\n",")"],"metadata":{"id":"fDgYQYb5wczh","executionInfo":{"status":"ok","timestamp":1753517142831,"user_tz":-330,"elapsed":1164,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.document_loaders import WebBaseLoader\n","from langchain_community.vectorstores import Chroma\n","\n","urls = [\n","    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n","    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n","    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n","]\n","\n","docs = [WebBaseLoader(url).load() for url in urls]\n","docs_list = [item for sublist in docs for item in sublist]\n","\n","text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n","    chunk_size=250, chunk_overlap=0\n",")\n","doc_splits = text_splitter.split_documents(docs_list)\n","#embeddings=Op\n","# Add to vectorDB\n","vectorstore = Chroma.from_documents(\n","    documents=doc_splits,\n","    collection_name=\"rag-chroma\",\n","    embedding=embeddings,\n",")\n","retriever = vectorstore.as_retriever()"],"metadata":{"id":"bf4qH3q_vRZW","executionInfo":{"status":"ok","timestamp":1753517184650,"user_tz":-330,"elapsed":4069,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["from langchain import hub\n","from langchain_core.output_parsers import StrOutputParser\n","\n","# Prompt\n","prompt = hub.pull(\"rlm/rag-prompt\")\n","\n","print(f\"---PROMPT--- {prompt}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VyThQSRgw8ro","executionInfo":{"status":"ok","timestamp":1753517235943,"user_tz":-330,"elapsed":113,"user":{"displayName":"200 project","userId":"16889346664957907140"}},"outputId":"9ad1072a-fda7-41e4-bd18-70458c9fd0a9"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["---PROMPT--- input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/langsmith/client.py:272: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# Post-processing\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","\n","# Chain\n","rag_chain = (\n","    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n","    | prompt\n","    | llm\n","    | StrOutputParser()\n",")"],"metadata":{"id":"klCDAtCYx9xu","executionInfo":{"status":"ok","timestamp":1753517397941,"user_tz":-330,"elapsed":24,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["from langchain_core.runnables import RunnablePassthrough\n","\n","question = \"tell me about agent memory.\"\n","generation = rag_chain.invoke(question)\n","print(generation)"],"metadata":{"id":"tah9oAEgyAKb","executionInfo":{"status":"ok","timestamp":1753517400713,"user_tz":-330,"elapsed":1283,"user":{"displayName":"200 project","userId":"16889346664957907140"}},"outputId":"910e5fa6-eca6-41b2-8b17-b451af012cd0","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Agent memory consists of short-term memory for in-context learning and long-term memory for retaining and recalling information over extended periods. The agent can utilize external APIs for additional information and leverage a retrieval model to inform its behavior based on relevance, recency, and importance. Memory types include sensory memory for retaining sensory information briefly after stimuli end.\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"0JNko7_MxqZC"},"execution_count":null,"outputs":[]}]}