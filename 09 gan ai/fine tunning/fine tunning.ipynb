{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPGIxxFU7J165mAPks3pniY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Perfect ‚Äî these are **core concepts in modern AI model training**. I‚Äôll explain them in **simple words** (like you‚Äôd answer in an interview), with a balance of **theory + intuition**.\n","\n","---\n","\n","## üîπ 1. Full Fine-Tuning\n","\n","üëâ Imagine a **pre-trained model** (like GPT or BERT) that already knows a lot.\n","\n","* **Full fine-tuning** = you take **all model parameters (billions of weights)** and update them using your data.\n","* It‚Äôs like **retraining the whole brain** of the model on your specific task.\n","\n","‚úÖ **Pros:** Very accurate because the whole model adapts.\n","‚ùå **Cons:** Needs a lot of **data, GPU memory, and time**.\n","\n","üí° Example: If GPT was pre-trained on general English text, full fine-tuning on medical text would make it a \"medical GPT\" but costs huge compute.\n","\n","---\n","\n","## üîπ 2. Parameter-Efficient Fine-Tuning (PEFT)\n","\n","Instead of changing **all parameters**, you change only **a small part** and keep the rest frozen.\n","This makes it **cheaper & faster**. Two main methods are **LoRA** and **QLoRA**.\n","\n","---\n","\n","### üî∏ LoRA (Low-Rank Adaptation)\n","\n","* Instead of updating the huge weight matrices, LoRA adds **tiny trainable matrices** (low-rank adapters) inside.\n","* The **big model stays frozen**, only the small adapters learn.\n","* Much cheaper: Instead of training 100B parameters, maybe only a few million.\n","\n","üí° Think of it like adding **new \"side memory chips\"** to a brain without rewriting the whole brain.\n","\n","---\n","\n","### üî∏ QLoRA (Quantized LoRA)\n","\n","* Same as LoRA, but with **extra memory-saving trick**:\n","\n","  * The big frozen model is stored in **compressed (quantized) form** (e.g., 4-bit instead of 16/32-bit).\n","  * Adapters are still trained in full precision.\n","\n","‚úÖ Even more **GPU-efficient** ‚Üí lets you fine-tune very large models on a single GPU.\n","\n","üí° Think of QLoRA like keeping the \"brain\" in a **compressed notebook** and only writing notes (adapters) in normal size.\n","\n","---\n","\n","## üîπ 3. RLHF (Reinforcement Learning with Human Feedback)\n","\n","* After fine-tuning, models might still give **bad or unsafe answers**.\n","* RLHF = teaching the model to **prefer answers humans like**.\n","\n","Steps:\n","\n","1. **Collect human preferences** ‚Üí e.g., humans rank which answer sounds better.\n","2. **Train a reward model** ‚Üí predicts human preference.\n","3. **Reinforce** the main model ‚Üí using reinforcement learning so it generates answers humans prefer.\n","\n","üí° Example: If a chatbot gives \"I don‚Äôt know\" vs. \"Here‚Äôs a polite explanation,\" humans prefer the second ‚Üí RLHF pushes the model to act that way.\n","\n","---\n","\n","## üîπ 4. RLAIF / RLVR (Variants of RLHF)\n","\n","You mentioned **RLVR** ‚Äî often people mean **RLAIF (Reinforcement Learning with AI Feedback)** or similar methods. Let me break it down:\n","\n","* **RLHF = humans** give feedback (expensive, slow).\n","* **RLAIF / RLVR = AI models** give feedback instead of humans.\n","\n","  * Example: Use GPT-4 as a \"judge\" to rank responses from a smaller model.\n","  * This automates the process and reduces human effort.\n","\n","üí° Think of it like asking a **teacher AI** to grade a **student AI‚Äôs answers** instead of a human teacher.\n","\n","---\n","\n","## üéØ Summary in One-Liners\n","\n","* **Full Fine-Tuning** ‚Üí update the whole model (expensive, powerful).\n","* **LoRA** ‚Üí train small adapter layers, freeze rest (efficient).\n","* **QLoRA** ‚Üí LoRA + model compression for even cheaper fine-tuning.\n","* **RLHF** ‚Üí make model align with **human preferences**.\n","* **RLAIF / RLVR** ‚Üí same idea, but **AI gives feedback** instead of humans.\n","\n","---\n","\n","‚ö° Now, do you want me to make a **comparison table** (Full vs LoRA vs QLoRA vs RLHF vs RLVR) that you can memorize for interviews?\n"],"metadata":{"id":"mu9UTN5S4QDC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"sR9Tw-3dxf_K"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Great question üëç Let‚Äôs break down **LoRA** and **QLoRA** fine-tuning parameters, what they mean, and when you should use them.\n","\n","---\n","\n","# üîπ 1. LoRA (Low-Rank Adaptation) Fine-Tuning Parameters\n","\n","LoRA inserts **trainable low-rank matrices** into specific layers of a frozen pre-trained model. Instead of updating all weights, it learns \"adapters\" that are much smaller.\n","\n","### Key Parameters in LoRA:\n","\n","1. **`r` (Rank)**\n","\n","   * Defines the size of the low-rank decomposition.\n","   * Higher `r` ‚Üí more capacity, but more memory.\n","   * Typical values: `4, 8, 16, 32`.\n","   * **When to use**:\n","\n","     * Small `r` (4‚Äì8) for lightweight adaptation.\n","     * Large `r` (16‚Äì32) if you have more GPU and want stronger adaptation.\n","\n","2. **`alpha` (Scaling factor)**\n","\n","   * Controls how much influence the LoRA updates have compared to the frozen base model.\n","   * Often set as `alpha = 2 * r` or similar.\n","   * Higher alpha ‚Üí stronger adaptation, but may risk overfitting.\n","\n","3. **`target_modules`**\n","\n","   * Which layers to apply LoRA to (e.g., `query`, `value`, `key`, or all linear layers).\n","   * Typically applied to **attention projection layers** (`q_proj`, `v_proj`) because they‚Äôre most impactful.\n","   * **When to use**:\n","\n","     * Small dataset ‚Üí fewer target modules (avoid overfitting).\n","     * Large dataset ‚Üí more target modules (better adaptation).\n","\n","4. **Dropout (LoRA dropout)**\n","\n","   * Adds dropout to LoRA layers.\n","   * Helps regularization when training on small datasets.\n","   * Typical values: `0.05 ‚Äì 0.1`.\n","\n","5. **Learning rate & optimizer**\n","\n","   * Since only LoRA parameters are trained, you can use a **slightly higher LR** than full fine-tuning.\n","   * Common range: `2e-4 ‚Äì 2e-3`.\n","\n","---\n","\n","# üîπ 2. QLoRA (Quantized LoRA) Fine-Tuning Parameters\n","\n","QLoRA = **Quantized LLM + LoRA adapters**.\n","\n","* Base model is quantized to **4-bit**, adapters are **16-bit or 32-bit**.\n","* Enables fine-tuning large models (e.g., 33B, 65B) on **a single GPU**.\n","\n","### Key Parameters in QLoRA:\n","\n","1. **Quantization Type (bits)**\n","\n","   * Typically **4-bit NormalFloat (NF4)** or `bnb_4bit`.\n","   * Lower precision ‚Üí smaller memory footprint.\n","   * NF4 performs better than INT4.\n","   * **When to use**:\n","\n","     * Use NF4 if possible (best trade-off).\n","     * Use pure int4 only if memory is extremely constrained.\n","\n","2. **`r`, `alpha`, `target_modules`, `dropout`**\n","\n","   * Same meaning as in LoRA.\n","   * Usually, **lower r (4‚Äì8)** is enough because the base model is large.\n","\n","3. **`bnb_4bit_compute_dtype` (compute precision)**\n","\n","   * Defines computation precision during forward/backward.\n","   * Common: `bfloat16` (bf16) or `float16`.\n","   * **When to use**:\n","\n","     * bf16 if your GPU supports it (A100, H100).\n","     * fp16 otherwise.\n","\n","4. **`bnb_4bit_quant_type` (quantization scheme)**\n","\n","   * Options: `nf4`, `fp4`.\n","   * `nf4` is preferred for stability.\n","\n","5. **Double Quantization (`bnb_4bit_use_double_quant`)**\n","\n","   * Quantizes quantization constants for extra memory savings.\n","   * Default: `True`.\n","   * Slightly reduces VRAM without major performance drop.\n","\n","6. **Learning Rate**\n","\n","   * Usually smaller than LoRA because quantization already introduces instability.\n","   * Common range: `1e-4 ‚Äì 5e-4`.\n","\n","---\n","\n","# üîπ When to Use LoRA vs QLoRA\n","\n","‚úÖ **Use LoRA** if:\n","\n","* Your base model fits in GPU memory in full precision.\n","* You want fast fine-tuning with less quantization complexity.\n","* Your model is medium size (1B‚Äì13B).\n","\n","‚úÖ **Use QLoRA** if:\n","\n","* You want to fine-tune **very large models (13B‚Äì70B)** on consumer GPUs (e.g., RTX 3090, A100 40GB).\n","* Memory efficiency is critical.\n","* You‚Äôre okay with a slight trade-off in training stability.\n","\n","---\n","\n","‚öñÔ∏è **Rule of Thumb**:\n","\n","* Small/medium model + enough GPU ‚Üí **LoRA**.\n","* Large model + limited GPU ‚Üí **QLoRA**.\n","\n","---\n"],"metadata":{"id":"pNDO-nVXxgd9"}},{"cell_type":"code","source":[],"metadata":{"id":"cgjOen8DxiRg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Got it ‚úÖ ‚Äî You want a **professional-level explanation** of the **main fine-tuning parameters** in **LoRA** and **QLoRA**, not just a table of values.\n","\n","I‚Äôll list the most **important parameters**, explain **what they control**, and give **best practices** for when/why to adjust them.\n","\n","---\n","\n","# üîπ Core Fine-Tuning Parameters in LoRA / QLoRA\n","\n","### 1. **`r` (Rank)**\n","\n","* **What it is:**\n","  Defines the size of the low-rank adapter matrices injected into the frozen layers.\n","  Larger `r` ‚Üí more trainable parameters ‚Üí higher capacity ‚Üí more GPU usage.\n","* **Professional Use:**\n","\n","  * `r = 4‚Äì8` ‚Üí lightweight adaptation (good for small data).\n","  * `r = 16‚Äì64` ‚Üí higher capacity (good for domain adaptation or bigger datasets).\n","  * Rule of thumb: start with **8 or 16**, increase only if underfitting.\n","\n","---\n","\n","### 2. **`alpha` (Scaling factor / LoRA alpha)**\n","\n","* **What it is:**\n","  Scales the LoRA update before adding it to the frozen base weights.\n","  Often set proportional to `r`.\n","* **Professional Use:**\n","\n","  * Common default: `alpha = 2 * r`.\n","  * Higher alpha makes LoRA updates dominate (risk of overfitting).\n","  * Lower alpha makes LoRA subtle (better for few-shot adaptation).\n","\n","---\n","\n","### 3. **`target_modules`**\n","\n","* **What it is:**\n","  Which layers LoRA adapters are applied to. Typically the **attention projections** (`q_proj`, `v_proj`, sometimes `k_proj`, `o_proj`).\n","* **Professional Use:**\n","\n","  * Minimal setup: `q_proj`, `v_proj` ‚Üí less memory, fast training.\n","  * Full attention: (`q,v,k,o_proj`) ‚Üí better for large datasets.\n","  * Advanced: also add to **MLP layers** if you need strong domain shift adaptation.\n","\n","---\n","\n","### 4. **LoRA Dropout**\n","\n","* **What it is:**\n","  Dropout applied only to the adapter layers (not the base model).\n","* **Professional Use:**\n","\n","  * Helps prevent overfitting on small datasets.\n","  * Typical values: `0.05 ‚Äì 0.1`.\n","  * For large datasets ‚Üí often set to `0.0`.\n","\n","---\n","\n","### 5. **Learning Rate**\n","\n","* **What it is:**\n","  Optimizer step size for updating LoRA/QLoRA adapter parameters.\n","* **Professional Use:**\n","\n","  * LoRA (full precision): `2e-4 ‚Äì 2e-3`.\n","  * QLoRA (quantized, more sensitive): `5e-5 ‚Äì 5e-4`.\n","  * Smaller dataset ‚Üí lower LR to avoid overfitting.\n","  * Larger dataset ‚Üí higher LR for faster convergence.\n","\n","---\n","\n","### 6. **Quantization Parameters (QLoRA-specific)**\n","\n","#### a) **`bnb_4bit_quant_type`**\n","\n","* **What it is:** Quantization scheme for 4-bit.\n","* **Options:** `nf4` (NormalFloat4, best), `fp4` (Float4).\n","* **Professional Use:**\n","\n","  * Always use `nf4` (better statistical properties).\n","  * Use `fp4` only if compatibility issues arise.\n","\n","#### b) **`bnb_4bit_compute_dtype`**\n","\n","* **What it is:** Precision for forward/backward computation.\n","* **Options:** `bf16`, `fp16`, `fp32`.\n","* **Professional Use:**\n","\n","  * `bf16` (preferred, stable, supported on A100/H100).\n","  * `fp16` if hardware doesn‚Äôt support bf16.\n","  * `fp32` only for extreme stability debugging (very slow).\n","\n","#### c) **`bnb_4bit_use_double_quant`**\n","\n","* **What it is:** Second level of quantization (quantizing quantization constants).\n","* **Professional Use:**\n","\n","  * `True` ‚Üí saves extra memory, negligible performance drop.\n","  * Recommended for large models (13B, 33B, 65B).\n","\n","---\n","\n","### 7. **Batch Size & Gradient Accumulation**\n","\n","* **What it is:** Controls how many samples are processed before optimizer update.\n","* **Professional Use:**\n","\n","  * Large batch ‚Üí more stable, needs more GPU.\n","  * Small batch ‚Üí use **gradient accumulation** to simulate larger batch.\n","  * Rule: keep **effective batch size \\~128‚Äì256 tokens** for stability.\n","\n","---\n","\n","### 8. **Optimizer**\n","\n","* **What it is:** Algorithm for updating adapter weights.\n","* **Professional Use:**\n","\n","  * `AdamW` with weight decay (`0.01`) is standard.\n","  * Use **paged AdamW (bitsandbytes)** in QLoRA to save memory.\n","\n","---\n","\n","### 9. **Scheduler**\n","\n","* **What it is:** How the learning rate changes during training.\n","* **Professional Use:**\n","\n","  * `cosine` decay or `linear` decay ‚Üí best for LoRA.\n","  * Use **warmup steps** (\\~5‚Äì10% of training) to stabilize early updates.\n","\n","---\n","\n","# üîπ Professional Recommendations\n","\n","* **Start simple**: `r=8, alpha=16, q/v_proj, dropout=0.05, LR=2e-4`.\n","* **Small dataset (few-shot)**: use **low r (4‚Äì8)**, **dropout > 0.05**, **lower LR**.\n","* **Large dataset (domain adaptation)**: use **higher r (16‚Äì32)**, expand `target_modules`, higher LR.\n","* **QLoRA for big models**: always use `nf4`, `bf16` if possible, double quantization on.\n","* **Monitor training**: if loss is flat ‚Üí increase `r` or LR; if overfitting ‚Üí lower alpha or add dropout.\n","\n","---\n","\n","‚öñÔ∏è **Summary in one line:**\n","üëâ *LoRA parameters (`r`, `alpha`, `target_modules`) control **capacity and where adaptation happens**, while QLoRA parameters (`quant_type`, `compute_dtype`, `double_quant`) control **efficiency and stability under quantization***.\n","\n","---\n","\n","Would you like me to now prepare a **cheat sheet (one-page reference card)** that lists these parameters with recommended defaults and tuning tips ‚Äî something you could keep handy when fine-tuning models?\n"],"metadata":{"id":"bk3jnX6yxoV2"}},{"cell_type":"code","source":[],"metadata":{"id":"QFMK5fN4xolC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perfect üëç Let‚Äôs build a **parameter recommendation table** for **LoRA vs QLoRA**, based on **model size** and **GPU memory availability**.\n","\n","---\n","\n","# üîπ Recommended Parameters for LoRA & QLoRA\n","\n","| Model Size | GPU Memory | Method    | Rank (`r`) | Alpha  | Target Modules                        | Dropout  | Quantization               | Compute Dtype       | LR Range    |\n","| ---------- | ---------- | --------- | ---------- | ------ | ------------------------------------- | -------- | -------------------------- | ------------------- | ----------- |\n","| **1B‚Äì3B**  | ‚â•12 GB     | **LoRA**  | 8‚Äì16       | 16‚Äì32  | `q_proj`, `v_proj` (attention only)   | 0.05     | ‚Äì (full precision)         | fp16/bf16           | 2e-4 ‚Äì 2e-3 |\n","| **7B**     | ‚â•24 GB     | **LoRA**  | 8‚Äì16       | 16‚Äì32  | `q_proj`, `v_proj` + maybe `k_proj`   | 0.05     | ‚Äì                          | fp16/bf16           | 2e-4 ‚Äì 1e-3 |\n","| **7B**     | 12‚Äì16 GB   | **QLoRA** | 4‚Äì8        | 16     | `q_proj`, `v_proj`                    | 0.05     | 4-bit (nf4)                | bf16 (if supported) | 1e-4 ‚Äì 5e-4 |\n","| **13B**    | ‚â•32 GB     | **LoRA**  | 16‚Äì32      | 32‚Äì64  | All attention layers (`q,v,k,o_proj`) | 0.05‚Äì0.1 | ‚Äì                          | bf16                | 2e-4 ‚Äì 8e-4 |\n","| **13B**    | 16‚Äì24 GB   | **QLoRA** | 8‚Äì16       | 16‚Äì32  | `q_proj`, `v_proj`                    | 0.05     | 4-bit (nf4)                | bf16                | 1e-4 ‚Äì 3e-4 |\n","| **33B**    | ‚â•64 GB     | **LoRA**  | 16‚Äì32      | 32‚Äì64  | All attention layers                  | 0.05‚Äì0.1 | ‚Äì                          | bf16                | 1e-4 ‚Äì 5e-4 |\n","| **33B**    | 24‚Äì32 GB   | **QLoRA** | 8‚Äì16       | 16‚Äì32  | `q_proj`, `v_proj`                    | 0.05     | 4-bit (nf4) + double quant | bf16                | 1e-4 ‚Äì 3e-4 |\n","| **65B**    | ‚â•128 GB    | **LoRA**  | 32‚Äì64      | 64‚Äì128 | All attention + MLP                   | 0.05‚Äì0.1 | ‚Äì                          | bf16                | 1e-4 ‚Äì 2e-4 |\n","| **65B**    | 40‚Äì48 GB   | **QLoRA** | 8‚Äì16       | 32‚Äì64  | `q_proj`, `v_proj`                    | 0.05     | 4-bit (nf4) + double quant | bf16                | 5e-5 ‚Äì 2e-4 |\n","\n","---\n","\n","# üîπ Key Takeaways\n","\n","* **Rank (`r`)** ‚Üí Higher for larger models, but keep it small (4‚Äì8) in QLoRA to save memory.\n","* **Alpha** ‚Üí Usually set around `2 * r`.\n","* **Target Modules** ‚Üí\n","\n","  * Minimal (`q_proj`, `v_proj`) for small datasets / limited GPU.\n","  * Full attention (`q,v,k,o_proj`) for large datasets.\n","  * Add MLP layers only if dataset is very large.\n","* **Dropout** ‚Üí Add when dataset is small (to avoid overfitting).\n","* **Learning Rate** ‚Üí Higher for LoRA, lower for QLoRA (since quantization adds instability).\n","* **Quantization** ‚Üí Always prefer `nf4` over `fp4`. Use double quantization for big models if VRAM is tight.\n","\n","---\n","\n","üëâ Do you also want me to make a **flowchart (decision tree)**: ‚ÄúGiven model size + GPU memory ‚Üí which fine-tuning method & parameters to choose‚Äù? That way you can decide in <10 seconds.\n"],"metadata":{"id":"LDBYPvWwxsIg"}},{"cell_type":"code","source":[],"metadata":{"id":"hkDiPBSxxsa-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perfect üëå let‚Äôs take all the **LoRA / QLoRA parameters** we just discussed and map them to **how they are actually used in the ü§ó PEFT (Parameter-Efficient Fine-Tuning) library**.\n","\n","---\n","\n","# üîπ PEFT Overview\n","\n","The Hugging Face **PEFT library** provides an easy way to apply LoRA/QLoRA on top of Hugging Face models (e.g., LLaMA, Falcon, GPT-J, etc.).\n","\n","You configure LoRA/QLoRA using the **`LoraConfig`** class (or `get_peft_model`), and PEFT handles injecting adapters into the model.\n","\n","---\n","\n","# üîπ Key Parameters in `LoraConfig` (PEFT)\n","\n","Here‚Äôs how the parameters we discussed earlier appear in PEFT:\n","\n","```python\n","from peft import LoraConfig, get_peft_model\n","\n","lora_config = LoraConfig(\n","    r=8,                          # Rank\n","    lora_alpha=16,                # Scaling factor\n","    target_modules=[\"q_proj\",\"v_proj\"],  # Where to inject adapters\n","    lora_dropout=0.05,            # Dropout inside adapters\n","    bias=\"none\",                  # Bias handling\n","    task_type=\"CAUSAL_LM\"         # Task type (Causal LM, Seq2Seq LM, Token Classification, etc.)\n",")\n","```\n","\n","---\n","\n","### 1. **`r`**\n","\n","* **PEFT param:** `r`\n","* Controls rank of the adapter matrices.\n","* Example: `r=8` ‚Üí lightweight; `r=32` ‚Üí more capacity.\n","\n","---\n","\n","### 2. **`lora_alpha`**\n","\n","* **PEFT param:** `lora_alpha`\n","* Scaling factor for LoRA updates. Usually `2 * r`.\n","* Example: `lora_alpha=16` for `r=8`.\n","\n","---\n","\n","### 3. **`target_modules`**\n","\n","* **PEFT param:** `target_modules`\n","* Defines which layers to apply LoRA to.\n","* Example:\n","\n","  ```python\n","  target_modules=[\"q_proj\", \"v_proj\"]\n","  ```\n","* For LLaMA/GPT-type models, you usually pick attention projections (`q_proj`, `v_proj`).\n","* For OPT/Bloom-type models, you may need `[\"query_key_value\"]`.\n","\n","---\n","\n","### 4. **`lora_dropout`**\n","\n","* **PEFT param:** `lora_dropout`\n","* Dropout applied to LoRA layers only.\n","* Typical values: `0.05 ‚Äì 0.1`.\n","\n","---\n","\n","### 5. **`bias`**\n","\n","* **PEFT param:** `bias`\n","* How PEFT handles biases in LoRA layers. Options:\n","\n","  * `\"none\"` ‚Üí don‚Äôt train any biases (default, most common).\n","  * `\"lora_only\"` ‚Üí only LoRA biases are trained.\n","  * `\"all\"` ‚Üí train all biases (not very parameter-efficient).\n","* Professional use: Keep `\"none\"` unless you have reason to fine-tune biases.\n","\n","---\n","\n","### 6. **`task_type`**\n","\n","* **PEFT param:** `task_type`\n","* Required to tell PEFT what task you‚Äôre training for.\n","* Options: `\"CAUSAL_LM\"`, `\"SEQ_2_SEQ_LM\"`, `\"TOKEN_CLS\"`, etc.\n","* Example:\n","\n","  ```python\n","  task_type=\"CAUSAL_LM\"\n","  ```\n","\n","---\n","\n","# üîπ QLoRA in PEFT\n","\n","With QLoRA, you use the **same `LoraConfig`** but load the base model in **4-bit precision** using `bitsandbytes`.\n","\n","Example:\n","\n","```python\n","from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n","\n","# Define quantization config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\",        # Quantization scheme\n","    bnb_4bit_compute_dtype=\"bfloat16\", # Compute precision\n","    bnb_4bit_use_double_quant=True     # Double quantization\n",")\n","\n","# Load quantized model\n","model = AutoModelForCausalLM.from_pretrained(\n","    \"meta-llama/Llama-2-7b-hf\",\n","    quantization_config=bnb_config,\n","    device_map=\"auto\"\n",")\n","\n","# Apply LoRA on top of quantized model\n","from peft import get_peft_model\n","peft_model = get_peft_model(model, lora_config)\n","```\n","\n","---\n","\n","# üîπ QLoRA-Specific Parameters in PEFT\n","\n","These are handled by `BitsAndBytesConfig` (from `transformers`), not inside `LoraConfig`:\n","\n","| Parameter                           | PEFT Usage (via BitsAndBytesConfig) | Explanation                                        |\n","| ----------------------------------- | ----------------------------------- | -------------------------------------------------- |\n","| `load_in_4bit=True`                 | Enables 4-bit quantization          | Base requirement for QLoRA                         |\n","| `bnb_4bit_quant_type=\"nf4\"`         | Quantization scheme                 | Use `nf4` for best results                         |\n","| `bnb_4bit_compute_dtype=\"bfloat16\"` | Compute precision                   | `bf16` preferred if GPU supports, otherwise `fp16` |\n","| `bnb_4bit_use_double_quant=True`    | Double quantization                 | Saves memory with negligible loss                  |\n","\n","---\n","\n","# üîπ Professional Defaults (PEFT Best Practices)\n","\n","* **LoRA (medium models, e.g. 7B):**\n","\n","  ```python\n","  r=8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"q_proj\",\"v_proj\"]\n","  ```\n","* **QLoRA (large models, e.g. 13B‚Äì70B):**\n","\n","  ```python\n","  r=4‚Äì8, lora_alpha=16, lora_dropout=0.05, target_modules=[\"q_proj\",\"v_proj\"]\n","  bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"bfloat16\", double_quant=True\n","  ```\n","* **Bias = \"none\"** unless explicitly needed.\n","* **Task type = \"CAUSAL\\_LM\"** for LLMs.\n","\n","---\n","\n","‚öñÔ∏è **In short**:\n","\n","* `LoraConfig` in PEFT controls **where and how adapters are trained**.\n","* `BitsAndBytesConfig` controls **how the base model is quantized** in QLoRA.\n","\n","---\n","\n","üëâ Do you want me to also prepare a **ready-to-use template script** (LoRA + QLoRA training loop with PEFT & HuggingFace Trainer), so you can directly adapt it for your models?\n"],"metadata":{"id":"NEjRiiuHzJyl"}},{"cell_type":"code","source":[],"metadata":{"id":"b3rN_StwzKH_"},"execution_count":null,"outputs":[]}]}