{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMU51DC2bHcBngWZxVbIZcO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1OnG1Vzhg9dF"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Absolutely! Here's the full project structure with **all necessary files** for your `reflexion_agent_system` Streamlit app. You can copy each file into your project folder and run it directly.\n","\n","---\n","\n","## ðŸ“ Folder Structure\n","\n","```\n","reflexion_agent_system/\n","â”œâ”€â”€ streamlit_app.py\n","â”œâ”€â”€ reflexion_agent_graph.py\n","â”œâ”€â”€ chains.py\n","â”œâ”€â”€ execute_tools.py\n","â”œâ”€â”€ schema.py\n","â”œâ”€â”€ requirements.txt\n","â””â”€â”€ .env\n","```\n","\n","---\n","\n","### 1. `.env`\n","\n","```env\n","OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n","```\n","\n","*Replace with your actual OpenAI API key.*\n","\n","---\n","\n","### 2. `requirements.txt`\n","\n","```txt\n","streamlit>=1.35.0\n","python-dotenv>=1.0.0\n","langchain-core>=0.1.40\n","langgraph>=0.0.32\n","langchain-openai>=0.1.5\n","langchain-community>=0.0.30\n","typing-extensions>=4.5.0\n","pydantic>=2.4\n","```\n","\n","---\n","\n","### 3. `schema.py`\n","\n","```python\n","from pydantic import BaseModel, Field\n","from typing import List\n","\n","class Reflection(BaseModel):\n","    missing: str = Field(description=\"Critique of what is missing.\")\n","    superfluous: str = Field(description=\"Critique of what is superfluous\")\n","\n","class AnswerQuestion(BaseModel):\n","    answer: str = Field(description=\"~250 word detailed answer to the question.\")\n","    search_queries: List[str] = Field(description=\"1â€‘3 search queries for researching improvements\")\n","    reflection: Reflection = Field(description=\"Your reflection on the initial answer.\")\n","\n","class ReviseAnswer(AnswerQuestion):\n","    references: List[str] = Field(description=\"Citations motivating your updated answer.\")\n","```\n","\n","---\n","\n","### 4. `chains.py`\n","\n","```python\n","from dotenv import load_dotenv\n","load_dotenv()\n","\n","import datetime\n","from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n","from langchain_openai import ChatOpenAI\n","from langchain_core.output_parsers.openai_tools import PydanticToolsParser, JsonOutputToolsParser\n","from schema import AnswerQuestion, ReviseAnswer\n","from langchain_core.messages import HumanMessage\n","\n","pydantic_parser = PydanticToolsParser(tools=[AnswerQuestion])\n","json_parser = JsonOutputToolsParser(return_id=True)\n","\n","actor_prompt_template = ChatPromptTemplate.from_messages([\n","    (\"system\",\n","     \"You are expert AI researcher.\\n\"\n","     \"Current time: {time}\\n\\n\"\n","     \"1. {first_instruction}\\n\"\n","     \"2. Reflect and critique your answer. Be severe to maximize improvement.\\n\"\n","     \"3. After reflection, **list 1-3 search queries separately**. Do not include them inside the reflection.\"),\n","    MessagesPlaceholder(variable_name=\"messages\"),\n","    (\"system\", \"Answer the user's question above using the required format.\")\n","]).partial(time=lambda: datetime.datetime.now().isoformat())\n","\n","first_responder_prompt = actor_prompt_template.partial(\n","    first_instruction=\"Provide a detailed ~250 word answer\"\n",")\n","revision_prompt = actor_prompt_template.partial(\n","    first_instruction=(\n","        \"Revise your previous answer using the new information.\\n\"\n","        \"- Use critiques to add missing info.\\n\"\n","        \"- Use numerical citations with a 'References' section.\\n\"\n","        \"- Keep answer â‰¤â€¯250 words.\"\n","    )\n",")\n","\n","llm = ChatOpenAI(model=\"gpt-4o\")\n","\n","first_responder_chain = first_responder_prompt | llm.bind_tools(\n","    tools=[AnswerQuestion], tool_choice=\"AnswerQuestion\"\n",")\n","\n","revisor_chain = revision_prompt | llm.bind_tools(\n","    tools=[ReviseAnswer], tool_choice=\"ReviseAnswer\"\n",")\n","```\n","\n","---\n","\n","### 5. `execute_tools.py`\n","\n","```python\n","import json\n","from typing import List\n","from langchain_core.messages import AIMessage, ToolMessage, BaseMessage, HumanMessage\n","from langchain_community.tools import TavilySearchResults\n","\n","tavily_tool = TavilySearchResults(max_results=5)\n","\n","def execute_tools(state: List[BaseMessage]) -> List[BaseMessage]:\n","    last = state[-1]\n","    if not hasattr(last, \"tool_calls\") or not last.tool_calls:\n","        return []\n","    tool_messages = []\n","    for call in last.tool_calls:\n","        name = call.get(\"name\")\n","        if name in (\"AnswerQuestion\", \"ReviseAnswer\"):\n","            call_id = call[\"id\"]\n","            queries = call[\"args\"].get(\"search_queries\", [])\n","            results = {}\n","            for q in queries:\n","                results[q] = tavily_tool.invoke(q)\n","            tool_messages.append(\n","                ToolMessage(content=json.dumps(results), tool_call_id=call_id)\n","            )\n","    return tool_messages\n","```\n","\n","---\n","\n","### 6. `reflexion_agent_graph.py`\n","\n","```python\n","from typing import List\n","from langchain_core.messages import BaseMessage, ToolMessage, HumanMessage\n","from langgraph.graph import MessageGraph, END\n","from chains import first_responder_chain, revisor_chain\n","from execute_tools import execute_tools\n","\n","graph = MessageGraph()\n","MAX_ITERS = 2\n","\n","graph.add_node(\"draft\", first_responder_chain)\n","graph.add_node(\"execute_tools\", execute_tools)\n","graph.add_node(\"revisor\", revisor_chain)\n","\n","graph.add_edge(\"draft\", \"execute_tools\")\n","graph.add_edge(\"execute_tools\", \"revisor\")\n","\n","def continue_loop(state: List[BaseMessage]):\n","    count = sum(isinstance(m, ToolMessage) for m in state)\n","    return END if count > MAX_ITERS else \"execute_tools\"\n","\n","graph.add_conditional_edges(\"revisor\", continue_loop)\n","graph.set_entry_point(\"draft\")\n","\n","app = graph.compile()\n","\n","if __name__ == \"__main__\":\n","    print(app.get_graph().draw_mermaid())\n","```\n","\n","---\n","\n","### 7. `streamlit_app.py`\n","\n","```python\n","import streamlit as st\n","from dotenv import load_dotenv\n","from langchain_core.messages import HumanMessage\n","from reflexion_agent_graph import app\n","load_dotenv()\n","\n","st.set_page_config(page_title=\"Reflexion Agent\", layout=\"wide\")\n","st.title(\"ðŸ§  Reflexion Agent System\")\n","st.markdown(\"Ask a question, then the system will reflect, revise, cite, and present final answer.\")\n","\n","question = st.text_area(\"Enter your question:\", height=150)\n","if st.button(\"Run System\") and question:\n","    with st.spinner(\"Processing...\"):\n","        result = app.invoke(HumanMessage(content=question))\n","\n","    last = result[-1]\n","    tool_call = last.tool_calls[0]\n","    args = tool_call[\"args\"]\n","\n","    st.subheader(\"âœ… Answer\")\n","    st.write(args[\"answer\"])\n","\n","    st.subheader(\"ðŸ§  Reflection\")\n","    st.markdown(f\"- **Missing:** {args['reflection']['missing']}\")\n","    st.markdown(f\"- **Superfluous:** {args['reflection']['superfluous']}\")\n","\n","    st.subheader(\"ðŸ”Ž Search Queries Used\")\n","    for q in args.get(\"search_queries\", []):\n","        st.write(f\"- {q}\")\n","\n","    if \"references\" in args:\n","        st.subheader(\"ðŸ“š References\")\n","        for idx, ref in enumerate(args[\"references\"], start=1):\n","            st.write(f\"[{idx}] {ref}\")\n","```\n","\n","---\n","\n","## ðŸš€ How to Run\n","\n","1. `cd` into the project folder.\n","2. Install dependencies:\n","\n","   ```bash\n","   pip install -r requirements.txt\n","   ```\n","3. Ensure `.env` has your OpenAI key.\n","4. Run:\n","\n","   ```bash\n","   streamlit run streamlit_app.py\n","   ```\n","\n","---\n","\n","You now have **all files ready** to launch a fully functioning reflexion agent with search, critique, revision, and citations. Let me know if you'd like to add logging, long-term memory, or deployment scripts!\n"],"metadata":{"id":"1Dkv6obhg985"}},{"cell_type":"code","source":[],"metadata":{"id":"3bnbEnuFg-aM"},"execution_count":null,"outputs":[]}]}