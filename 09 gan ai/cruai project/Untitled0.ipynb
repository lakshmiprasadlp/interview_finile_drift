{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMBDIsxOFqAe4uSC2BCTi85"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["%%capture\n","!pip install crewai langchain-community langchain-openai requests duckduckgo-search chromadb\n"],"metadata":{"id":"cImT-nknxzUC","executionInfo":{"status":"ok","timestamp":1754406622287,"user_tz":-330,"elapsed":58481,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"_HLhrtkayLoL"}},{"cell_type":"code","source":["import os\n","from google.colab import userdata\n","\n","os.environ['NEWSAPI_KEY'] = userdata.get('NEWSAPI_KEY')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"-lWCk3LXyMTv","executionInfo":{"status":"ok","timestamp":1754406687906,"user_tz":-330,"elapsed":7055,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from crewai import Agent, Task, Crew, Process\n","from langchain_openai import ChatOpenAI\n","from langchain_core.retrievers import BaseRetriever\n","from langchain_openai import OpenAIEmbeddings\n","from langchain.tools import tool\n","from langchain_community.document_loaders import WebBaseLoader\n","import requests, os\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import Chroma\n","from langchain_community.tools import DuckDuckGoSearchRun\n"],"metadata":{"id":"taCFAuvnx_Ts","executionInfo":{"status":"ok","timestamp":1754406859233,"user_tz":-330,"elapsed":11,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Tools\n"],"metadata":{"id":"JoP8IEN4y3YH"}},{"cell_type":"code","source":["from langchain.tools import Tool\n","\n","# === Define Tool Functions ===\n","\n","def search_news_db(query: str):\n","    API_KEY = os.getenv('NEWSAPI_KEY')\n","    base_url = \"https://newsapi.org/v2/everything\"\n","    params = {\n","        'q': query,\n","        'sortBy': 'publishedAt',\n","        'apiKey': API_KEY,\n","        'language': 'en',\n","        'pageSize': 5,\n","    }\n","\n","    response = requests.get(base_url, params=params)\n","    if response.status_code != 200:\n","        return \"Failed to retrieve news.\"\n","\n","    articles = response.json().get('articles', [])\n","    all_splits = []\n","    for article in articles:\n","        loader = WebBaseLoader(article['url'])\n","        docs = loader.load()\n","\n","        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","        splits = text_splitter.split_documents(docs)\n","        all_splits.extend(splits)\n","\n","    if all_splits:\n","        vectorstore = Chroma.from_documents(all_splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n","        retriever = vectorstore.similarity_search(query)\n","        return retriever\n","    else:\n","        return \"No content available for processing.\"\n","\n","\n","def get_news_db(query: str):\n","    vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n","    retriever = vectorstore.similarity_search(query)\n","    return retriever\n"],"metadata":{"id":"LvlvBL7Vz5b0","executionInfo":{"status":"ok","timestamp":1754407084173,"user_tz":-330,"elapsed":38,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["search_news_tool = Tool(\n","    name=\"SearchNewsDB\",\n","    func=search_news_db,\n","    description=\"Fetch and store latest news articles based on a query into a vector DB\"\n",")\n","\n","get_news_tool = Tool(\n","    name=\"GetNews\",\n","    func=get_news_db,\n","    description=\"Retrieve stored news articles from vector DB using a query\"\n",")\n"],"metadata":{"id":"zikbvvp2y5mj","executionInfo":{"status":"ok","timestamp":1754407110874,"user_tz":-330,"elapsed":15,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# Agent\n"],"metadata":{"id":"3z_lnZ9hzItf"}},{"cell_type":"code","source":["news_search_agent = Agent(\n","    role='News Seacher',\n","    goal='Generate key points for each news article from the latest news',\n","    backstory='Expert in analysing and generating key points from news content for quick updates.',\n","    tools=[search_news_tool],\n","    allow_delegation=True,\n","    verbose=True,\n","    llm=llm\n",")\n","\n","writer_agent = Agent(\n","    role='Writer',\n","    goal='Identify all the topics received. Use the Get News Tool to verify each topic. Use the Search tool for detailed exploration. Summarise in depth.',\n","    backstory='Expert in crafting engaging narratives from complex information.',\n","    tools=[get_news_tool, search_tool],\n","    allow_delegation=True,\n","    verbose=True,\n","    llm=llm\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"sOKFBiVezIeq","executionInfo":{"status":"error","timestamp":1754407125384,"user_tz":-330,"elapsed":66,"user":{"displayName":"200 project","userId":"16889346664957907140"}},"outputId":"0a8124bf-61e0-4392-cc7c-69dd39a4ca7b"},"execution_count":13,"outputs":[{"output_type":"error","ename":"ValidationError","evalue":"1 validation error for Agent\ntools.0\n  Input should be a valid dictionary or instance of BaseTool [type=model_type, input_value=Tool(name='SearchNewsDB',...s_db at 0x7bceae8eaca0>), input_type=Tool]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3766837766.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m news_search_agent = Agent(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'News Seacher'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Generate key points for each news article from the latest news'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbackstory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Expert in analysing and generating key points from news content for quick updates.'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msearch_news_tool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n","\u001b[0;31mValidationError\u001b[0m: 1 validation error for Agent\ntools.0\n  Input should be a valid dictionary or instance of BaseTool [type=model_type, input_value=Tool(name='SearchNewsDB',...s_db at 0x7bceae8eaca0>), input_type=Tool]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"]}]},{"cell_type":"code","source":[],"metadata":{"id":"B7fcGYcZzIbK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SO2zrH-yzIYk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0b5jFCFdzIV6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"Ls21dgrtxxrS","executionInfo":{"status":"error","timestamp":1754406737114,"user_tz":-330,"elapsed":219,"user":{"displayName":"200 project","userId":"16889346664957907140"}},"outputId":"2eea3242-3163-45a9-a2d5-9c1431bf5d3d"},"outputs":[{"output_type":"error","ename":"ValidationError","evalue":"1 validation error for Agent\ntools.0\n  Input should be a valid dictionary or instance of BaseTool [type=model_type, input_value=StructuredTool(name='News...news at 0x7bceae8e9760>), input_type=StructuredTool]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3539280618.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# 2. Creating Agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m news_search_agent = Agent(\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0mrole\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'News Seacher'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mgoal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Generate key points for each news article from the latest news'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n","\u001b[0;31mValidationError\u001b[0m: 1 validation error for Agent\ntools.0\n  Input should be a valid dictionary or instance of BaseTool [type=model_type, input_value=StructuredTool(name='News...news at 0x7bceae8e9760>), input_type=StructuredTool]\n    For further information visit https://errors.pydantic.dev/2.11/v/model_type"]}],"source":["from crewai import Agent, Task, Crew, Process\n","from langchain_openai import ChatOpenAI\n","from langchain_core.retrievers import BaseRetriever\n","from langchain_openai import OpenAIEmbeddings\n","from langchain.tools import tool\n","from langchain_community.document_loaders import WebBaseLoader\n","import requests, os\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain_community.vectorstores import Chroma\n","from langchain_community.tools import DuckDuckGoSearchRun\n","\n","embedding_function = OpenAIEmbeddings()\n","llm = ChatOpenAI(model=\"gpt-4-turbo-preview\",)\n","\n","# Tool 1 : Save the news articles in a database\n","class SearchNewsDB:\n","    @tool(\"News DB Tool\")\n","    def news(query: str):\n","        \"\"\"Fetch news articles and process their contents.\"\"\"\n","        API_KEY = os.getenv('NEWSAPI_KEY')  # Fetch API key from environment variable\n","        base_url = \"https://newsapi.org/v2/everything\"\n","\n","        params = {\n","            'q': query,\n","            'sortBy': 'publishedAt',\n","            'apiKey': API_KEY,\n","            'language': 'en',\n","            'pageSize': 5,\n","        }\n","\n","        response = requests.get(base_url, params=params)\n","        if response.status_code != 200:\n","            return \"Failed to retrieve news.\"\n","\n","        articles = response.json().get('articles', [])\n","        all_splits = []\n","        for article in articles:\n","            # Assuming WebBaseLoader can handle a list of URLs\n","            loader = WebBaseLoader(article['url'])\n","            docs = loader.load()\n","\n","            text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n","            splits = text_splitter.split_documents(docs)\n","            all_splits.extend(splits)  # Accumulate splits from all articles\n","\n","        # Index the accumulated content splits if there are any\n","        if all_splits:\n","            vectorstore = Chroma.from_documents(all_splits, embedding=embedding_function, persist_directory=\"./chroma_db\")\n","            retriever = vectorstore.similarity_search(query)\n","            return retriever\n","        else:\n","            return \"No content available for processing.\"\n","\n","# Tool 2 : Get the news articles from the database\n","class GetNews:\n","    @tool(\"Get News Tool\")\n","    def news(query: str) -> str:\n","        \"\"\"Search Chroma DB for relevant news information based on a query.\"\"\"\n","        vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embedding_function)\n","        retriever = vectorstore.similarity_search(query)\n","        return retriever\n","\n","# Tool 3 : Search for news articles on the web\n","search_tool = DuckDuckGoSearchRun()\n","\n","# 2. Creating Agents\n","news_search_agent = Agent(\n","    role='News Seacher',\n","    goal='Generate key points for each news article from the latest news',\n","    backstory='Expert in analysing and generating key points from news content for quick updates.',\n","    tools=[SearchNewsDB().news],\n","    allow_delegation=True,\n","    verbose=True,\n","    llm=llm\n",")\n","\n","writer_agent = Agent(\n","    role='Writer',\n","    goal='Identify all the topics received. Use the Get News Tool to verify the each topic to search. Use the Search tool for detailed exploration of each topic. Summarise the retrieved information in depth for every topic.',\n","    backstory='Expert in crafting engaging narratives from complex information.',\n","    tools=[GetNews().news, search_tool],\n","    allow_delegation=True,\n","    verbose=True,\n","    llm=llm\n",")\n","\n","# 3. Creating Tasks\n","news_search_task = Task(\n","    description='Search for AI 2024 and create key points for each news.',\n","    agent=news_search_agent,\n","    tools=[SearchNewsDB().news]\n",")\n","\n","writer_task = Task(\n","    description=\"\"\"\n","    Go step by step.\n","    Step 1: Identify all the topics received.\n","    Step 2: Use the Get News Tool to verify the each topic by going through one by one.\n","    Step 3: Use the Search tool to search for information on each topic one by one.\n","    Step 4: Go through every topic and write an in-depth summary of the information retrieved.\n","    Don't skip any topic.\n","    \"\"\",\n","    agent=writer_agent,\n","    context=[news_search_task],\n","    tools=[GetNews().news, search_tool]\n",")\n","\n","# 4. Creating Crew\n","news_crew = Crew(\n","    agents=[news_search_agent, writer_agent],\n","    tasks=[news_search_task, writer_task],\n","    process=Process.sequential,\n","    manager_llm=llm\n",")\n","\n","# Execute the crew to see RAG in action\n","result = news_crew.kickoff()\n","print(result)"]},{"cell_type":"code","source":[],"metadata":{"id":"dwH6hIxhylJK"},"execution_count":null,"outputs":[]}]}