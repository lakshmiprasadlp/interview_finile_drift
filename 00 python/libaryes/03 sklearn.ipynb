{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPA4UqntjCILfc8JJzEfhPj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mUMBSRpmWUGC"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Perfect! Let’s create a **Master-Level Scikit-Learn (sklearn) Reference Sheet** that you can use as a single-page ultimate cheat sheet for **interviews, projects, and learning**. I’ll structure it **categorically with brief explanations, syntax, and pro tips**.\n","\n","---\n","\n","# **Ultimate Scikit-Learn Reference Sheet**\n","\n","---\n","\n","## **1️⃣ Data Preprocessing / Feature Engineering**\n","\n","| Function / Class                                    | Module          | Purpose                                    | Example                                        |\n","| --------------------------------------------------- | --------------- | ------------------------------------------ | ---------------------------------------------- |\n","| `StandardScaler()`                                  | `preprocessing` | Standardize features (mean=0, std=1)       | `X_scaled = StandardScaler().fit_transform(X)` |\n","| `MinMaxScaler()`                                    | `preprocessing` | Scale to \\[0,1]                            | `X_scaled = MinMaxScaler().fit_transform(X)`   |\n","| `RobustScaler()`                                    | `preprocessing` | Scale using median & IQR                   | Resistant to outliers                          |\n","| `Normalizer()`                                      | `preprocessing` | Normalize rows to unit norm                | Useful for text data                           |\n","| `Binarizer(threshold=0.0)`                          | `preprocessing` | Convert values > threshold to 1 else 0     | `Binarizer(0.5)`                               |\n","| `PolynomialFeatures(degree=2)`                      | `preprocessing` | Generate polynomial & interaction features | Feature expansion                              |\n","| `OneHotEncoder()`                                   | `preprocessing` | Convert categorical to one-hot             | `OneHotEncoder().fit_transform(df_cat)`        |\n","| `LabelEncoder()`                                    | `preprocessing` | Encode labels to integers                  | `LabelEncoder().fit_transform(y)`              |\n","| `LabelBinarizer()`                                  | `preprocessing` | Convert multi-class labels to binary       | For multi-class logistic regression            |\n","| `PowerTransformer(method='yeo-johnson')`            | `preprocessing` | Make data Gaussian-like                    | Reduce skewness                                |\n","| `QuantileTransformer(output_distribution='normal')` | `preprocessing` | Map data to uniform / normal               | Robust scaling                                 |\n","\n","---\n","\n","## **2️⃣ Model Selection & Evaluation**\n","\n","| Function / Class                                     | Module            | Purpose                              |\n","| ---------------------------------------------------- | ----------------- | ------------------------------------ |\n","| `train_test_split()`                                 | `model_selection` | Split data into train/test           |\n","| `KFold(n_splits=5)`                                  | `model_selection` | K-fold cross-validation              |\n","| `StratifiedKFold()`                                  | `model_selection` | Maintain class distribution in folds |\n","| `cross_val_score(estimator, X, y, cv=5)`             | `model_selection` | Evaluate model using CV              |\n","| `cross_val_predict()`                                | `model_selection` | Get cross-validated predictions      |\n","| `GridSearchCV(estimator, param_grid)`                | `model_selection` | Exhaustive hyperparameter tuning     |\n","| `RandomizedSearchCV(estimator, param_distributions)` | `model_selection` | Randomized hyperparameter tuning     |\n","| `ShuffleSplit()`                                     | `model_selection` | Random splitting multiple times      |\n","| `StratifiedShuffleSplit()`                           | `model_selection` | Maintain class balance in splits     |\n","\n","---\n","\n","## **3️⃣ Supervised Learning**\n","\n","### **Regression**\n","\n","| Model                     | Module         | Use Case                  | Syntax                                    |\n","| ------------------------- | -------------- | ------------------------- | ----------------------------------------- |\n","| LinearRegression          | `linear_model` | Continuous prediction     | `LinearRegression().fit(X,y)`             |\n","| Ridge                     | `linear_model` | L2-regularized regression | `Ridge(alpha=1.0)`                        |\n","| Lasso                     | `linear_model` | L1-regularized regression | `Lasso(alpha=0.1)`                        |\n","| ElasticNet                | `linear_model` | L1+L2 regularization      | `ElasticNet(alpha=0.1,l1_ratio=0.5)`      |\n","| DecisionTreeRegressor     | `tree`         | Non-linear regression     | `DecisionTreeRegressor(max_depth=5)`      |\n","| RandomForestRegressor     | `ensemble`     | Ensemble trees            | `RandomForestRegressor(n_estimators=100)` |\n","| GradientBoostingRegressor | `ensemble`     | Boosted regression        | `GradientBoostingRegressor()`             |\n","\n","### **Classification**\n","\n","| Model                      | Module         | Use Case                  | Syntax                                     |\n","| -------------------------- | -------------- | ------------------------- | ------------------------------------------ |\n","| LogisticRegression         | `linear_model` | Binary / multi-class      | `LogisticRegression()`                     |\n","| DecisionTreeClassifier     | `tree`         | Tree-based classification | `DecisionTreeClassifier()`                 |\n","| RandomForestClassifier     | `ensemble`     | Ensemble classifier       | `RandomForestClassifier(n_estimators=100)` |\n","| GradientBoostingClassifier | `ensemble`     | Boosted classifier        | `GradientBoostingClassifier()`             |\n","| KNeighborsClassifier       | `neighbors`    | kNN classifier            | `KNeighborsClassifier(n_neighbors=5)`      |\n","| SVC                        | `svm`          | SVM classifier            | `SVC(kernel='rbf')`                        |\n","| GaussianNB                 | `naive_bayes`  | Probabilistic classifier  | `GaussianNB()`                             |\n","| AdaBoostClassifier         | `ensemble`     | Boosted ensemble          | `AdaBoostClassifier()`                     |\n","\n","---\n","\n","## **4️⃣ Unsupervised Learning / Dimensionality Reduction**\n","\n","| Model / Function        | Module          | Use Case                            |\n","| ----------------------- | --------------- | ----------------------------------- |\n","| KMeans                  | `cluster`       | Partition into k clusters           |\n","| DBSCAN                  | `cluster`       | Density-based clustering            |\n","| AgglomerativeClustering | `cluster`       | Hierarchical clustering             |\n","| MeanShift               | `cluster`       | Cluster by shifting centroids       |\n","| PCA                     | `decomposition` | Reduce dimensions linearly          |\n","| TruncatedSVD            | `decomposition` | Sparse dimensionality reduction     |\n","| KernelPCA               | `decomposition` | Non-linear dimensionality reduction |\n","| NMF                     | `decomposition` | Non-negative matrix factorization   |\n","\n","---\n","\n","## **5️⃣ Metrics / Model Evaluation**\n","\n","### **Regression**\n","\n","| Metric                | Purpose            | Syntax                                |\n","| --------------------- | ------------------ | ------------------------------------- |\n","| mean\\_squared\\_error  | Measures MSE       | `mean_squared_error(y_true, y_pred)`  |\n","| mean\\_absolute\\_error | Measures MAE       | `mean_absolute_error(y_true, y_pred)` |\n","| r2\\_score             | Variance explained | `r2_score(y_true, y_pred)`            |\n","\n","### **Classification**\n","\n","| Metric                 | Purpose                              | Syntax                                  |\n","| ---------------------- | ------------------------------------ | --------------------------------------- |\n","| accuracy\\_score        | Correct prediction rate              | `accuracy_score(y_true, y_pred)`        |\n","| precision\\_score       | True positives / predicted positives | `precision_score(y_true, y_pred)`       |\n","| recall\\_score          | True positives / actual positives    | `recall_score(y_true, y_pred)`          |\n","| f1\\_score              | Harmonic mean of precision & recall  | `f1_score(y_true, y_pred)`              |\n","| roc\\_auc\\_score        | Area under ROC curve                 | `roc_auc_score(y_true, y_prob)`         |\n","| confusion\\_matrix      | Count of TP, FP, FN, TN              | `confusion_matrix(y_true, y_pred)`      |\n","| classification\\_report | Summary metrics                      | `classification_report(y_true, y_pred)` |\n","\n","### **Clustering**\n","\n","| Metric                    | Purpose                               | Syntax                               |\n","| ------------------------- | ------------------------------------- | ------------------------------------ |\n","| silhouette\\_score         | Cluster cohesion                      | `silhouette_score(X, labels)`        |\n","| davies\\_bouldin\\_score    | Cluster separation                    | `davies_bouldin_score(X, labels)`    |\n","| calinski\\_harabasz\\_score | Ratio of inter/intra-cluster distance | `calinski_harabasz_score(X, labels)` |\n","\n","---\n","\n","## **6️⃣ Pipeline & Utilities**\n","\n","| Function / Class                | Purpose                                     |\n","| ------------------------------- | ------------------------------------------- |\n","| Pipeline(steps)                 | Combine preprocessing + model in one object |\n","| ColumnTransformer(transformers) | Apply transformations to specific columns   |\n","| FunctionTransformer(func)       | Custom feature transformation               |\n","| joblib.dump() / joblib.load()   | Save / Load models                          |\n","| make\\_pipeline()                | Shortcut to create pipelines                |\n","\n","---\n","\n","## **7️⃣ Feature Selection**\n","\n","| Function / Class                        | Purpose                                  |\n","| --------------------------------------- | ---------------------------------------- |\n","| SelectKBest(score\\_func, k=10)          | Top-k features based on univariate stats |\n","| RFE(estimator, n\\_features\\_to\\_select) | Recursive feature elimination            |\n","| VarianceThreshold(threshold=0.0)        | Remove low variance features             |\n","| SelectFromModel(estimator, threshold)   | Select features using model importance   |\n","\n","---\n","\n"],"metadata":{"id":"NusJDB8AWV3-"}},{"cell_type":"code","source":[],"metadata":{"id":"Ubpt3hmHWXUw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perfect! You want a **master-level coding reference sheet for scikit-learn**, including **actual code examples** for all major tasks: preprocessing, model building, evaluation, pipelines, and feature selection. Here’s a **concise, ready-to-use Python reference sheet**.\n","\n","---\n","\n","# **Master-Level Scikit-Learn Coding Reference Sheet**\n","\n","---\n","\n","## **1️⃣ Data Preprocessing / Feature Engineering**\n","\n","```python\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n","from sklearn.preprocessing import OneHotEncoder, LabelEncoder, PolynomialFeatures\n","from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n","\n","# Standardization\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)\n","\n","# Min-Max Scaling\n","X_minmax = MinMaxScaler().fit_transform(X)\n","\n","# Robust Scaling (resistant to outliers)\n","X_robust = RobustScaler().fit_transform(X)\n","\n","# Row-wise Normalization\n","X_norm = Normalizer().fit_transform(X)\n","\n","# Polynomial Features\n","poly = PolynomialFeatures(degree=2)\n","X_poly = poly.fit_transform(X)\n","\n","# One-Hot Encoding (categorical)\n","ohe = OneHotEncoder(sparse=False)\n","X_ohe = ohe.fit_transform(X_cat)\n","\n","# Label Encoding (target)\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","\n","# Power Transform (Yeo-Johnson)\n","pt = PowerTransformer()\n","X_pt = pt.fit_transform(X)\n","\n","# Quantile Transformation (Gaussian)\n","qt = QuantileTransformer(output_distribution='normal')\n","X_qt = qt.fit_transform(X)\n","```\n","\n","---\n","\n","## **2️⃣ Train/Test Split & Cross-Validation**\n","\n","```python\n","from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n","from sklearn.model_selection import cross_val_score, cross_val_predict\n","from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n","\n","# Split data\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# K-Fold CV\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","for train_idx, val_idx in kf.split(X):\n","    X_tr, X_val = X[train_idx], X[val_idx]\n","    y_tr, y_val = y[train_idx], y[val_idx]\n","\n","# Cross-validated score\n","scores = cross_val_score(estimator, X, y, cv=5)\n","\n","# Hyperparameter tuning\n","grid = GridSearchCV(estimator, param_grid={'C':[0.1,1,10]}, cv=5)\n","grid.fit(X_train, y_train)\n","best_model = grid.best_estimator_\n","```\n","\n","---\n","\n","## **3️⃣ Supervised Learning: Regression**\n","\n","```python\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, LogisticRegression\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","\n","# Linear Regression\n","lr = LinearRegression()\n","lr.fit(X_train, y_train)\n","y_pred = lr.predict(X_test)\n","\n","# Ridge Regression\n","ridge = Ridge(alpha=1.0)\n","ridge.fit(X_train, y_train)\n","\n","# Lasso Regression\n","lasso = Lasso(alpha=0.1)\n","lasso.fit(X_train, y_train)\n","\n","# ElasticNet\n","en = ElasticNet(alpha=0.1, l1_ratio=0.5)\n","en.fit(X_train, y_train)\n","\n","# Decision Tree Regressor\n","dt = DecisionTreeRegressor(max_depth=5)\n","dt.fit(X_train, y_train)\n","\n","# Random Forest Regressor\n","rf = RandomForestRegressor(n_estimators=100)\n","rf.fit(X_train, y_train)\n","\n","# Gradient Boosting Regressor\n","gb = GradientBoostingRegressor()\n","gb.fit(X_train, y_train)\n","```\n","\n","---\n","\n","## **4️⃣ Supervised Learning: Classification**\n","\n","```python\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","\n","# Logistic Regression\n","logr = LogisticRegression()\n","logr.fit(X_train, y_train)\n","y_pred = logr.predict(X_test)\n","\n","# Decision Tree\n","dtc = DecisionTreeClassifier(max_depth=5)\n","dtc.fit(X_train, y_train)\n","\n","# Random Forest\n","rfc = RandomForestClassifier(n_estimators=100)\n","rfc.fit(X_train, y_train)\n","\n","# Gradient Boosting\n","gbc = GradientBoostingClassifier()\n","gbc.fit(X_train, y_train)\n","\n","# AdaBoost\n","abc = AdaBoostClassifier(n_estimators=100)\n","abc.fit(X_train, y_train)\n","\n","# KNN\n","knn = KNeighborsClassifier(n_neighbors=5)\n","knn.fit(X_train, y_train)\n","\n","# SVM\n","svc = SVC(kernel='rbf', probability=True)\n","svc.fit(X_train, y_train)\n","\n","# Naive Bayes\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","```\n","\n","---\n","\n","## **5️⃣ Unsupervised Learning / Dimensionality Reduction**\n","\n","```python\n","from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, MeanShift\n","from sklearn.decomposition import PCA, TruncatedSVD, KernelPCA, NMF\n","\n","# K-Means Clustering\n","kmeans = KMeans(n_clusters=3)\n","labels = kmeans.fit_predict(X)\n","\n","# DBSCAN\n","db = DBSCAN(eps=0.5, min_samples=5)\n","labels_db = db.fit_predict(X)\n","\n","# Agglomerative Clustering\n","agg = AgglomerativeClustering(n_clusters=3)\n","labels_agg = agg.fit_predict(X)\n","\n","# PCA\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","\n","# Truncated SVD (for sparse matrices)\n","svd = TruncatedSVD(n_components=2)\n","X_svd = svd.fit_transform(X)\n","\n","# Kernel PCA\n","kpca = KernelPCA(n_components=2, kernel='rbf')\n","X_kpca = kpca.fit_transform(X)\n","\n","# NMF\n","nmf = NMF(n_components=2)\n","X_nmf = nmf.fit_transform(X)\n","```\n","\n","---\n","\n","## **6️⃣ Metrics & Evaluation**\n","\n","```python\n","from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n","from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n","\n","# Regression Metrics\n","mse = mean_squared_error(y_test, y_pred)\n","mae = mean_absolute_error(y_test, y_pred)\n","r2 = r2_score(y_test, y_pred)\n","\n","# Classification Metrics\n","acc = accuracy_score(y_test, y_pred)\n","prec = precision_score(y_test, y_pred)\n","rec = recall_score(y_test, y_pred)\n","f1 = f1_score(y_test, y_pred)\n","roc = roc_auc_score(y_test, y_prob)\n","cm = confusion_matrix(y_test, y_pred)\n","report = classification_report(y_test, y_pred)\n","\n","# Clustering Metrics\n","sil = silhouette_score(X, labels)\n","db = davies_bouldin_score(X, labels)\n","ch = calinski_harabasz_score(X, labels)\n","```\n","\n","---\n","\n","## **7️⃣ Pipelines & Column Transformers**\n","\n","```python\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","\n","# Preprocessing pipeline for numeric and categorical\n","numeric_features = ['age','income']\n","categorical_features = ['gender','city']\n","\n","numeric_transformer = Pipeline(steps=[('scaler', StandardScaler())])\n","categorical_transformer = Pipeline(steps=[('onehot', OneHotEncoder())])\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', numeric_transformer, numeric_features),\n","        ('cat', categorical_transformer, categorical_features)\n","    ]\n",")\n","\n","# Full pipeline with classifier\n","clf = Pipeline(steps=[('preprocessor', preprocessor),\n","                      ('classifier', RandomForestClassifier())])\n","clf.fit(X_train, y_train)\n","y_pred = clf.predict(X_test)\n","```\n","\n","---\n","\n","## **8️⃣ Feature Selection**\n","\n","```python\n","from sklearn.feature_selection import SelectKBest, f_classif, RFE, VarianceThreshold, SelectFromModel\n","from sklearn.ensemble import RandomForestClassifier\n","\n","# Select top-k features using ANOVA F-test\n","skb = SelectKBest(score_func=f_classif, k=5)\n","X_new = skb.fit_transform(X, y)\n","\n","# Recursive Feature Elimination (RFE)\n","rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=5)\n","X_rfe = rfe.fit_transform(X, y)\n","\n","# Variance Threshold\n","vt = VarianceThreshold(threshold=0.1)\n","X_vt = vt.fit_transform(X)\n","\n","# Select from model (feature importance)\n","sfm = SelectFromModel(estimator=RandomForestClassifier(), threshold='median')\n","X_sfm = sfm.fit_transform(X, y)\n","```\n","\n","---\n","\n"],"metadata":{"id":"hWNrXpdEWsL1"}},{"cell_type":"code","source":[],"metadata":{"id":"mCMfjubDWtQk"},"execution_count":null,"outputs":[]}]}