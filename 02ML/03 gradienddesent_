{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvpRNe9305ivrwSdhiIdSS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Let's explore **Gradient Descent** in depth â€” including what it is, how it works, how itâ€™s used in Linear Regression, and how it's affected by learning rate and convergence.\n","\n","---\n","\n","## ğŸ”¸ What is Gradient Descent?\n","\n","**Gradient Descent** is an **optimization algorithm** used to minimize a **cost/loss function** by iteratively updating parameters (weights).\n","\n","> It finds the **minimum value** of a function (like MSE in Linear Regression).\n","\n","---\n","\n","## ğŸ”¹ Why Gradient Descent?\n","\n","In Linear Regression, the **Mean Squared Error (MSE)** is the loss function:\n","\n","$$\n","J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2\n","$$\n","\n","We want to **minimize this cost function**, and gradient descent helps us do that by finding the best parameters $\\theta_0$ and $\\theta_1$.\n","\n","---\n","\n","## ğŸ”¸ Gradient Descent Algorithm\n","\n","### âœ… General Formula:\n","\n","$$\n","\\theta_j = \\theta_j - \\alpha \\cdot \\frac{\\partial J}{\\partial \\theta_j}\n","$$\n","\n","* $\\theta_j$: The parameter weâ€™re updating (e.g., weight)\n","* $\\alpha$: Learning rate\n","* $\\frac{\\partial J}{\\partial \\theta_j}$: Partial derivative (gradient)\n","\n","### âœ… For Linear Regression:\n","\n","For simple regression $h_\\theta(x) = \\theta_0 + \\theta_1x$, we update:\n","\n","$$\n","\\theta_0 = \\theta_0 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})\n","$$\n","\n","$$\n","\\theta_1 = \\theta_1 - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)}) \\cdot x^{(i)}\n","$$\n","\n","---\n","\n","## ğŸ”¸ Types of Gradient Descent\n","\n","| Type                 | Description                                | Example             |\n","| -------------------- | ------------------------------------------ | ------------------- |\n","| **Batch**            | Uses **all data** for each update          | Stable, but slow    |\n","| **Stochastic (SGD)** | Updates using **one data point at a time** | Faster, but noisy   |\n","| **Mini-batch**       | Uses **a subset** (mini-batch) of data     | Best of both worlds |\n","\n","---\n","\n","## ğŸ”¸ Learning Rate (Î±)\n","\n","* **Too small** â†’ Slow convergence\n","* **Too large** â†’ Overshooting, may never converge\n","* **Right size** â†’ Smooth and fast convergence\n","\n","---\n","\n","## âœ… Simple Python Implementation (No sklearn)\n","\n","```python\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Dataset\n","X = np.array([1, 2, 3, 4, 5])\n","y = np.array([2, 4, 6, 8, 10])  # Perfect linear relation: y = 2x\n","\n","# Initialization\n","m = len(X)\n","theta0 = 0\n","theta1 = 0\n","alpha = 0.01\n","epochs = 1000\n","\n","# Gradient Descent\n","for _ in range(epochs):\n","    y_pred = theta0 + theta1 * X\n","    error = y_pred - y\n","\n","    # Update rules\n","    theta0 -= alpha * (1/m) * np.sum(error)\n","    theta1 -= alpha * (1/m) * np.sum(error * X)\n","\n","print(f\"Theta0 (Intercept): {theta0:.4f}\")\n","print(f\"Theta1 (Slope): {theta1:.4f}\")\n","\n","# Predict and Plot\n","plt.scatter(X, y, label='Data')\n","plt.plot(X, theta0 + theta1 * X, color='red', label='Prediction')\n","plt.title(\"Gradient Descent Linear Regression\")\n","plt.xlabel(\"X\")\n","plt.ylabel(\"y\")\n","plt.legend()\n","plt.grid(True)\n","plt.show()\n","```\n","\n","---\n","\n","## ğŸ”¸ Visual Intuition\n","\n","* Gradient = **slope of loss function**\n","* Gradient Descent = walk **downhill** until you reach the **minimum**\n","* Loss becomes smaller with every update (if learning rate is good)\n","\n","---\n","\n","## ğŸ§  Summary\n","\n","| Term                  | Meaning                                   |\n","| --------------------- | ----------------------------------------- |\n","| **Gradient**          | Derivative: Direction of steepest slope   |\n","| **Descent**           | Move in direction to **minimize** cost    |\n","| **Learning Rate (Î±)** | Step size of each update                  |\n","| **Convergence**       | Reaching minimum cost (when gradient â‰ˆ 0) |\n","\n","---\n","\n","Would you like to visualize **cost function vs. iterations** or implement **Polynomial Regression with gradient descent** too?\n"],"metadata":{"id":"CUB44mEe92Ou"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vkYAnzGm9TdO","executionInfo":{"status":"ok","timestamp":1753805723747,"user_tz":-330,"elapsed":5,"user":{"displayName":"200 project","userId":"16889346664957907140"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Absolutely! Letâ€™s break these terms down into **very simple words** ğŸ‘‡\n","\n","---\n","\n","## ğŸ”¹ 1. **Loss Function** ğŸ¤•\n","\n","### ğŸ“Œ Simple Meaning:\n","\n","The **loss function** tells you **how wrong** your model is for **one data point**.\n","\n","### ğŸ’¬ Example:\n","\n","You guessed a house price to be â‚¹50L but the actual price is â‚¹55L.\n","Your loss is the difference: â‚¹5L (error). Thatâ€™s your **loss** for that one example.\n","\n","---\n","\n","## ğŸ”¹ 2. **Cost Function** ğŸ’°\n","\n","### ğŸ“Œ Simple Meaning:\n","\n","The **cost function** is the **average of all losses** over the **entire dataset**.\n","It tells how bad the model is on **all data points combined**.\n","\n","### ğŸ’¬ Example:\n","\n","You guessed prices for 100 houses.\n","The cost function tells you how far off your guesses are **on average**.\n","\n","âœ… You want this number to be **as small as possible**.\n","\n","---\n","\n","## ğŸ”¹ 3. **Iteration** ğŸ”\n","\n","### ğŸ“Œ Simple Meaning:\n","\n","One **iteration** is one update to your model's weights using gradient descent.\n","\n","### ğŸ’¬ Example:\n","\n","* You see your model is wrong.\n","* You slightly adjust it.\n","* Thatâ€™s **one iteration**.\n","\n","You repeat this many times to improve accuracy.\n","\n","---\n","\n","## ğŸ”¹ 4. **Epoch** ğŸ”„\n","\n","### ğŸ“Œ Simple Meaning:\n","\n","One **epoch** means your model has seen **all the data once**.\n","\n","### ğŸ’¬ Example:\n","\n","You have 10,000 photos.\n","If your model looks at **all 10,000 once**, thatâ€™s **1 epoch**.\n","\n","Usually, you need **multiple epochs** (e.g., 100 epochs) to train a good model.\n","\n","---\n","\n","## ğŸ¯ Summary Table\n","\n","| Term              | Simple Definition                    | Example                                 |\n","| ----------------- | ------------------------------------ | --------------------------------------- |\n","| **Loss Function** | Error on **1 sample**                | Predicted â‚¹50L, actual â‚¹55L â†’ loss = 5L |\n","| **Cost Function** | Average error over **all samples**   | Avg loss over 100 houses                |\n","| **Iteration**     | **One update** to the model          | Update weights once                     |\n","| **Epoch**         | Model saw **all training data once** | 1 pass through entire dataset           |\n","\n","---\n","\n","Let me know if you want a visual cartoon-style diagram to remember these easily!\n"],"metadata":{"id":"LyKQZQ3w90Z8"}},{"cell_type":"code","source":[],"metadata":{"id":"jkGVjFfE93U7"},"execution_count":null,"outputs":[]}]}