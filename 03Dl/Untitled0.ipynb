{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOGFLUZ57hyLYDmRlF5XbWX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d83e0s_8NztT"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Great — here’s a **clear and simple explanation** for each topic in **3. Training & Optimization**, with **interview-focused points and examples**.\n","\n","---\n","\n","### **3.1 Weight Initialization**\n","\n","**What:** How we set the initial weights of a neural network before training starts.\n","\n","**Why it matters:**\n","Bad initialization can cause vanishing or exploding gradients. Good initialization makes training faster and more stable.\n","\n","**Common techniques:**\n","\n","* **Zero Initialization:**\n","  Set all weights to 0 → Bad! Makes all neurons behave the same (no learning).\n","\n","* **Random Initialization:**\n","  Random small numbers → Better than zero, but may still cause instability.\n","\n","* **Xavier/Glorot Initialization (for sigmoid/tanh):**\n","  Keeps variance same across layers.\n","  $Var(W) = 2 / (n_in + n_out)$\n","\n","* **He Initialization (for ReLU):**\n","  Designed for ReLU activations.\n","  $Var(W) = 2 / n_in$\n","\n","✅ **Interview tip:** Be ready to explain why Xavier is better for sigmoid/tanh and He for ReLU.\n","\n","---\n","\n","### **3.2 Batch Normalization & Layer Normalization**\n","\n","#### **Batch Normalization (BN)**\n","\n","**What:** Normalize the inputs of each layer using mean & std from the **batch**.\n","\n","**Why:**\n","\n","* Reduces internal covariate shift.\n","* Speeds up training.\n","* Acts like a regularizer (less overfitting).\n","\n","**How:**\n","\n","1. Normalize: subtract batch mean, divide by batch std.\n","2. Scale & shift with learnable parameters.\n","\n","**Used in:** CNNs, MLPs.\n","\n","#### **Layer Normalization**\n","\n","**What:** Normalize across **features** (not batch).\n","\n","**When to use:**\n","\n","* Works better in RNNs, Transformers (because batch size can vary or be 1).\n","\n","✅ **Interview Tip:** \"BatchNorm depends on batch size. LayerNorm doesn’t — useful for sequential data.\"\n","\n","---\n","\n","### **3.3 Transfer Learning & Fine-Tuning**\n","\n","#### **Transfer Learning**\n","\n","**What:** Use a pre-trained model (like ResNet, BERT) and reuse its knowledge for a new task.\n","\n","**Example:**\n","Use ImageNet-trained ResNet to classify medical images.\n","\n","* **Freeze** early layers (they learn general features).\n","* Replace final layer to match your new task.\n","\n","#### **Fine-Tuning**\n","\n","**What:** Train the entire pre-trained model (or last few layers) **a little more** on your dataset.\n","\n","✅ **Interview Tip:**\n","Explain when you’d freeze vs fine-tune:\n","\n","* Freeze: Small dataset.\n","* Fine-tune: Large enough dataset to adjust features.\n","\n","---\n","\n","### **3.4 Meta-Learning & Few-Shot Learning**\n","\n","#### **Meta-Learning (\"Learning to Learn\")**\n","\n","**What:** A model learns **how to learn new tasks quickly**.\n","\n","**Why:** In real-world tasks, we may have **few examples** per task.\n","\n","**Example:**\n","Train a model on many tasks with few examples (like different character styles), so it generalizes fast.\n","\n","#### **Few-Shot Learning**\n","\n","**What:** Learning a new task with very few labeled examples (e.g., 1-shot, 5-shot learning).\n","\n","**Example:**\n","Learn to classify new animals with 1 image per class.\n","\n","**Popular approaches:**\n","\n","* **Siamese Networks**\n","* **Prototypical Networks**\n","* **MAML (Model-Agnostic Meta-Learning)**\n","\n","✅ **Interview Tip:**\n","If asked: “How would you classify new categories with 1 example per class?” — mention **Few-shot + Meta-Learning**.\n","\n","---\n","\n","Would you like a quick **PDF of all sections you’ve covered so far** for revision?\n"],"metadata":{"id":"0KMNHIT2N0WB"}},{"cell_type":"code","source":[],"metadata":{"id":"1wMhLBNtN0mU"},"execution_count":null,"outputs":[]}]}