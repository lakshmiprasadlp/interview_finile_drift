{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNs4COydI5VSyPzErVuuLtW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Here's a comprehensive list of **Keras classes, functions, and modules** used for building **Artificial Neural Networks (ANNs)**:\n","\n","---\n","\n","## ðŸ”¹ 1. **Modules in Keras for ANN**\n","\n","```python\n","from tensorflow import keras\n","from tensorflow.keras import layers, models, optimizers, losses, metrics, initializers, callbacks\n","```\n","\n","---\n","\n","## ðŸ”¹ 2. **Core ANN Classes and Functions**\n","\n","### âœ… **Model Building**\n","\n","* `keras.models.Sequential()`\n","* `keras.models.Model()` *(for Functional API)*\n","\n","### âœ… **Layers (Common for ANN)**\n","\n","* `layers.Dense()` â€“ Fully connected layer\n","* `layers.Dropout()` â€“ Dropout regularization\n","* `layers.BatchNormalization()` â€“ Batch normalization\n","* `layers.Activation()` â€“ Apply activation function\n","* `layers.Input()` â€“ Input layer (used in Functional API)\n","* `layers.Flatten()` â€“ Flattens input\n","\n","---\n","\n","## ðŸ”¹ 3. **Activation Functions**\n","\n","* `\"relu\"` â€“ Rectified Linear Unit\n","* `\"sigmoid\"` â€“ Sigmoid activation\n","* `\"tanh\"` â€“ Hyperbolic tangent\n","* `\"softmax\"` â€“ Used for multi-class classification\n","* `\"linear\"` â€“ No activation\n","\n","> You can also use `keras.activations` directly:\n","\n","```python\n","keras.activations.relu, sigmoid, softmax, tanh, linear\n","```\n","\n","---\n","\n","## ðŸ”¹ 4. **Loss Functions**\n","\n","* `losses.MeanSquaredError()` or `'mse'`\n","* `losses.MeanAbsoluteError()` or `'mae'`\n","* `losses.BinaryCrossentropy()` or `'binary_crossentropy'`\n","* `losses.CategoricalCrossentropy()` or `'categorical_crossentropy'`\n","* `losses.SparseCategoricalCrossentropy()`\n","\n","---\n","\n","## ðŸ”¹ 5. **Optimizers**\n","\n","* `optimizers.SGD()`\n","* `optimizers.Adam()`\n","* `optimizers.RMSprop()`\n","* `optimizers.Adagrad()`\n","* `optimizers.Adamax()`\n","* `optimizers.Nadam()`\n","\n","---\n","\n","## ðŸ”¹ 6. **Metrics**\n","\n","* `metrics.Accuracy()`\n","* `metrics.BinaryAccuracy()`\n","* `metrics.CategoricalAccuracy()`\n","* `metrics.AUC()`\n","* `metrics.MeanAbsoluteError()`\n","* `'accuracy'`, `'mae'`, `'mse'`, etc.\n","\n","---\n","\n","## ðŸ”¹ 7. **Model Compilation & Training**\n","\n","* `.compile(optimizer, loss, metrics)`\n","* `.fit(X, y, epochs=..., batch_size=...)`\n","* `.evaluate(X_test, y_test)`\n","* `.predict(X_input)`\n","\n","---\n","\n","## ðŸ”¹ 8. **Callbacks**\n","\n","* `callbacks.EarlyStopping()`\n","* `callbacks.ModelCheckpoint()`\n","* `callbacks.TensorBoard()`\n","* `callbacks.ReduceLROnPlateau()`\n","\n","---\n","\n","## ðŸ”¹ 9. **Initializers**\n","\n","Used to initialize weights of layers:\n","\n","* `initializers.Zeros()`\n","* `initializers.Ones()`\n","* `initializers.RandomNormal()`\n","* `initializers.HeNormal()` *(Good for ReLU)*\n","* `initializers.GlorotUniform()` *(Xavier init)*\n","\n","---\n","\n","## ðŸ”¹ 10. **Utilities**\n","\n","* `keras.utils.to_categorical()` â€“ Converts class vectors to binary class matrix\n","* `keras.utils.plot_model()` â€“ Visualize model architecture\n","\n","---\n","\n","### ðŸ”¸ Example of ANN using `Sequential` API:\n","\n","```python\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = Sequential([\n","    Dense(64, activation='relu', input_shape=(10,)),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=20, batch_size=32)\n","```\n","\n","---\n","\n","Would you like the **same list as a PDF or visual diagram**?\n"],"metadata":{"id":"ElxBsSEP-NXe"}},{"cell_type":"markdown","source":["Hereâ€™s a comprehensive list of key **Keras classes, functions, and modules** used for building **Artificial Neural Networks (ANNs)** in TensorFlow (as of **TensorFlow 2.x**):\n","\n","---\n","\n","### **1. Core Layers (Building Blocks of ANN)**\n","- **`Dense`** â€“ Fully connected layer (`tf.keras.layers.Dense`).\n","- **`Input`** â€“ Defines input layer/specification (`tf.keras.Input`).\n","- **`Flatten`** â€“ Flattens input (e.g., for CNN â†’ Dense transition).\n","- **`Dropout`** â€“ Regularization by randomly dropping units (`tf.keras.layers.Dropout`).\n","- **`BatchNormalization`** â€“ Normalizes activations for stable training.\n","\n","---\n","\n","### **2. Activation Functions**\n","- **`activations`** module (`tf.keras.activations`):\n","  - `relu`, `sigmoid`, `softmax`, `tanh`, `leaky_relu`, `elu`, `selu`, `swish`.\n","- **Usage in layers**:  \n","  ```python\n","  Dense(64, activation='relu')  \n","  ```\n","\n","---\n","\n","### **3. Model Classes**\n","- **`Sequential`** â€“ Linear stack of layers (`tf.keras.Sequential`).\n","- **`Model`** â€“ Functional API for complex architectures (`tf.keras.Model`).\n","- **`Subclassing`** â€“ Custom models via `tf.keras.Model` subclassing.\n","\n","---\n","\n","### **4. Loss Functions (`tf.keras.losses`)**\n","- **Regression**: `MeanSquaredError` (MSE), `MeanAbsoluteError` (MAE).\n","- **Classification**:  \n","  - `BinaryCrossentropy` (for binary classification),  \n","  - `CategoricalCrossentropy` (multi-class),  \n","  - `SparseCategoricalCrossentropy` (integer labels).  \n","- **Custom Loss**: Define a function and pass it to `model.compile()`.\n","\n","---\n","\n","### **5. Optimizers (`tf.keras.optimizers`)**\n","- **`SGD`** â€“ Stochastic Gradient Descent (with momentum support).\n","- **`Adam`**, `AdamW` â€“ Adaptive momentum-based optimizer.\n","- **`RMSprop`** â€“ Root Mean Square Propagation.\n","- **`Adagrad`**, `Adadelta`, `Nadam` â€“ Other adaptive optimizers.\n","\n","---\n","\n","### **6. Metrics (`tf.keras.metrics`)**\n","- **`Accuracy`**, `BinaryAccuracy`, `CategoricalAccuracy`.\n","- **`Precision`**, `Recall`, `AUC` (for imbalanced data).\n","- **`MeanSquaredError`**, `MeanAbsoluteError` (regression).\n","- Custom metrics via subclassing `tf.keras.metrics.Metric`.\n","\n","---\n","\n","### **7. Initializers (`tf.keras.initializers`)**\n","- **`HeNormal`**, `HeUniform` â€“ For ReLU activations.\n","- **`GlorotNormal`**, `GlorotUniform` (Xavier initialization).\n","- **`RandomNormal`**, `RandomUniform`, `Zeros`, `Ones`.\n","\n","---\n","\n","### **8. Regularizers (`tf.keras.regularizers`)**\n","- **`L1`**, `L2` â€“ L1/L2 weight regularization.\n","- **`L1L2`** â€“ Combined L1 + L2.\n","- Used in layers:  \n","  ```python\n","  Dense(64, kernel_regularizer=l2(0.01))\n","  ```\n","\n","---\n","\n","### **9. Callbacks (`tf.keras.callbacks`)**\n","- **`EarlyStopping`** â€“ Stops training if no improvement.\n","- **`ModelCheckpoint`** â€“ Saves model weights.\n","- **`TensorBoard`** â€“ Logs for visualization.\n","- **`LearningRateScheduler`** â€“ Adjusts LR dynamically.\n","- **`CSVLogger`** â€“ Saves training history to CSV.\n","\n","---\n","\n","### **10. Data Preprocessing & Utilities**\n","- **`tf.keras.utils.to_categorical`** â€“ One-hot encodes labels.\n","- **`tf.keras.utils.normalize`** â€“ Normalizes data.\n","- **`tf.keras.preprocessing.sequence`** â€“ For time-series data.\n","- **`tf.keras.preprocessing.image.ImageDataGenerator`** â€“ Augmentation (though prefer `tf.data` in modern TF).\n","\n","---\n","\n","### **11. Training & Evaluation**\n","- **`model.compile()`** â€“ Configures optimizer, loss, metrics.\n","- **`model.fit()`** â€“ Trains the model.\n","- **`model.evaluate()`** â€“ Evaluates on test data.\n","- **`model.predict()`** â€“ Generates predictions.\n","- **`model.save()`** / **`tf.keras.models.load_model()`** â€“ Saves/loads models.\n","\n","---\n","\n","### **12. Advanced Layers (Beyond Simple ANN)**\n","- **Embedding** â€“ For categorical data (`tf.keras.layers.Embedding`).\n","- **RNN/LSTM/GRU** â€“ Recurrent layers (though typically for sequences).\n","- **Conv1D/2D/3D** â€“ Convolutional layers (for CNNs).\n","\n","---\n","\n","### **Example Code Snippet:**\n","```python\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","model = Sequential([\n","    Dense(128, activation='relu', input_shape=(input_dim,)),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dense(1, activation='sigmoid')  # Binary classification\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n","```\n","\n","---\n","\n","### **Where to Find Documentation?**\n","- Official Docs: [https://keras.io/api/](https://keras.io/api/)  \n","- TensorFlow Keras: [https://www.tensorflow.org/api_docs/python/tf/keras](https://www.tensorflow.org/api_docs/python/tf/keras)\n","\n","This list covers the essentials for **ANNs** (MLPs). For CNNs/RNNs, additional layers are used. Let me know if you need details on a specific topic!"],"metadata":{"id":"n7JyAWdy-J1L"}},{"cell_type":"code","source":[],"metadata":{"id":"FIZu9oeN-Kwd"},"execution_count":null,"outputs":[]}]}